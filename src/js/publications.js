export const publications = [
  {
    title: 'CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks',
    authors: 'Yixuan Li*, Yutang Lin*, Jieming Cui, Tengyu Liu, Wei Liang, Yixin Zhu, Siyuan Huang',
    publisher: 'CoRL 2025',
    description: '',
    links: {
      image: '/project/corl25_clone/CLONE_teaser.png',
      arxiv: 'https://arxiv.org/abs/2506.08931',
      video: 'https://vimeo.com/1092360484',
      code: 'https://github.com/humanoid-clone/CLONE/',
      web: 'https://humanoid-clone.github.io/'
    }
  },
  {
    title: 'LiteAT: A Data-Lightweight and User-Adaptive VR Telepresence System for Remote Education',
    authors: 'Yuxin Shen , Wei Liang , Jianzhu Ma',
    publisher: 'TVCG 2025 (Special Issue on IEEE ISMAR 2025) (CCF A)',
    description: 'This paper introduces LiteAT, a data-lightweight and user-adaptive VR telepresence system, to enable real-time, immersive learning experiences.',
    links: {
      image: '',
      web: 'https://sites.google.com/view/lite-adaptive-telepresence',
    }
  },
  {
    title: 'Env-Mani: Quadrupedal Robot Loco-Manipulation with Environment-in-the-Loop',
    authors: 'Yixuan Li, Zan Wang, Wei Liang',
    publisher: 'IROS 2025 (CCF C)',
    description: '',
    links: {
      image: '/project/iros25_env/Env-manip_teaser.png',
      video: 'https://www.youtube.com/watch?v=8BAaLCI8cis',
      web: 'https://sites.google.com/view/env-mani',
    }
  },
  {
    title: 'R2G: Reasoning to Ground in 3D Scenes',
    authors: 'Yixuan Li, Zan Wang, Wei Liang',
    publisher: 'Pattern Recognition 2025 (CCF B)',
    description: '',
    links: {
      image: '/project/pr25_r2g/R2G_teaser.png',
      paper: 'https://www.sciencedirect.com/science/article/abs/pii/S0031320325003887',
      arxiv: 'https://arxiv.org/abs/2408.13499',
      video: 'https://www.youtube.com/watch?v=Y2dhg3cyiqY',
      code: 'https://github.com/yixxuan-li/R2G',
      web: 'https://sites.google.com/view/reasoning-to-ground',
    }
  },
  {
    title: 'FloNa: Floor Plan Guided Embodied Visual Navigation',
    authors: 'Jiaxin Li, Weiqi Huang, Zan Wang, Wei Liang, Huijun Di, Feng Liu',
    publisher: 'AAAI 2025 (CCF A), <font style="color:#FF0800">Oral</font>',
    description: 'We propose FloNa, a novel navigation task that leverages prior floor plans for guidance. To efficiently solve FloNa, we introduce FloDiff, a diffusion-based policy framework equipped with a localization module.',
    links: {
      image: '/project/aaai25_flona/teaser.jpg',
      paper: 'https://ojs.aaai.org/index.php/AAAI/article/view/33601',
      arxiv: 'https://arxiv.org/abs/2412.18335',
      code: 'https://github.com/GauleeJX/flodiff',
      web: 'https://gauleejx.github.io/flona/'
    }
  },
  {
    title: "X's Day: Personality-Driven Virtual Human Behavior Generation",
    authors: 'Haoyang Li, Zan Wang, Wei Liang, Yizhuo Wang',
    publisher: 'IEEE TVCG (Special Issue on IEEE VR 2025) (CCF A)',
    description: 'This paper introduces a novel task focused on autoregressively generating long-term behaviors for virtual agents, guided by specific personality traits and contextual elements within 3D environments.',
    links: {
      image: '/project/vr25_behavior/teaser.jpg',
      paper: 'https://behavior.agent-x.cn/static/files/paper.pdf',
      video: 'https://www.youtube.com/watch?v=Me8Pd3kAxmM',
      web: 'https://behavior.agent-x.cn/'
    }
  },
  {
    title: 'Visual Loop Closure Detection with Thorough Temporal and Spatial Context Exploitation',
    authors: 'Jiaxin Li*, Zan Wang*, Huijun Di, Jian Li, Wei Liang',
    publisher: 'IROS 2024 (CCF C), <font style="color:#FF0800">Oral Pitch</font>',
    description: 'We propose to leverage the global temporal and local spatial-temporal information for loop closure detection.',
    links: {
      image: '/project/iros24_tosa/teaser.png',
      paper: 'https://gauleejx.github.io/IROS2024_TOSA/static/pdfs/TOSA.pdf',
      video: 'https://www.youtube.com/watch?v=CmxsxJgDCXo',
      web: 'https://gauleejx.github.io/IROS2024_TOSA/'
    }
  },
  {
    title: 'Mastering Scene Rearrangement with Expert-assisted Curriculum Learning and Adaptive Trade-Off Tree-Search',
    authors: 'Zan Wang*, Hanqing Wang*, Wei Liang',
    publisher: 'IROS 2024 (CCF C), <font style="color:#FF0800">Oral Pitch</font>',
    description: 'We solve scene rearrangement planning by introducing an expert-assisted curriculum learning paradigm and a tree-search-based planner enhanced by an adaptive trade-off strategy.',
    links: {
      image: '/project/iros24_plato/teaser.png',
      paper: 'https://pl-ato.github.io/static/pdfs/paper.pdf',
      video: 'https://www.youtube.com/watch?v=gUYR1Hk8eVI',
      code: 'https://github.com/pl-ato/PLATO',
      web: 'https://pl-ato.github.io/'
    }
  },
  {
    title: 'Prompt-guided Precise Audio Editing with Diffusion Models',
    authors: 'Manjie Xu, Chenxing Li, Dan Su, Wei Liang, Dong Yu',
    publisher: 'ICML 2024 (CCF A)',
    description: 'We present a novel editing approach guided by textual prompt which serves as a general module for diffusion models and enables precise editing of multimodal generation content.',
    links: {
      image: '/project/icml24_ppae/teaser.jpg',
      paper: 'https://openreview.net/pdf?id=kQ1dwuheR0',
      arxiv: 'https://arxiv.org/pdf/2406.04350',
      web: 'https://sites.google.com/view/icml24-ppae'
    }
  },
  {
    title: 'Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance',
    authors: 'Zan Wang, Yixin Chen, Baoxiong Jia, Puhao Li, Jinlu Zhang, Jingze Zhang, Tengyu Liu, Yixin Zhu, Wei Liang, Siyuan Huang',
    publisher: 'CVPR 2024 (CCF A), <font style="color:#FF0800">Highlight</font>',
    description: 'We introduce a novel two-stage framework that employs scene affordance as an intermediate representation, effectively linking 3D scene grounding and conditional motion generation.',
    links: {
      image: '/project/cvpr24_afford/teaser.gif',
      paper: 'https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Move_as_You_Say_Interact_as_You_Can_Language-guided_Human_CVPR_2024_paper.pdf',
      arxiv: 'https://arxiv.org/abs/2403.18036',
      video: 'https://www.youtube.com/watch?v=emT0FHDYY1U',
      code: 'https://github.com/afford-motion/afford-motion',
      web: 'https://afford-motion.github.io/',
    }
  },
  {
    title: 'Context-Aware Head-and-Eye Motion Generation with Diffusion Model',
    authors: 'Yuxin Shen, Manjie Xu, Wei Liang',
    publisher: 'IEEE VR 2024 (CCF A)',
    description: 'We use a diffusion model to generate realistic head-and-eye motions, which are harmoniously aliged with simulated environments.',
    links: {
      image: '/project/vr24_eye/teaser.jpg',
      paper: '/project/vr24_eye/VR2024_Eye.pdf',
      video: 'https://www.youtube.com/watch?v=_Hf872Ce0v4',
      code: 'https://github.com/SYXcandice/CAHE',
      web: 'https://sites.google.com/view/context-aware-generation',
    }
  },
  {
    title: 'Active Reasoning in an Open-World Environment',
    authors: 'Manjie Xu, Guangyuan Jiang, Wei Liang, Chi Zhang, Yixin Zhu',
    publisher: 'NeurIPS 2023 (CCF A)',
    description: 'We build artificial agents that possess the ability to actively explore, accumulate, and reason using both newfound and existing information to tackle incomplete-information questions, functioning like AI detectives.',
    links: {
      image: '/project/neurips23_conan/teaser.jpg',
      paper: 'https://proceedings.neurips.cc/paper_files/paper/2023/file/2712b17bb58ea5b2b65c45857b024744-Paper-Conference.pdf',
      arxiv: 'https://arxiv.org/pdf/2311.02018.pdf',
      video: 'https://vimeo.com/878540519',
      web: 'https://sites.google.com/view/conan-active-reasoning',
    }
  },
  {
    title: 'Interactive Visual Reasoning under Uncertainty',
    authors: 'Manjie Xu*, Guangyuan Jiang*, Wei Liang, Chi Zhang, Yixin Zhu',
    publisher: 'NeurIPS 2023 (CCF A)',
    description: 'We test modern artificial agents whether they can quickly resolve uncertainty by generating hypotheses and testing them via active trials when encountering a novel phenomenon accompanied by ambiguous cause-effect relations.',
    links: {
      image: '/project/neurips23_ivre/teaser.jpg',
      paper: 'https://proceedings.neurips.cc/paper_files/paper/2023/file/844f722dbbcb27933ff5baf58a1f00c8-Paper-Datasets_and_Benchmarks.pdf',
      arxiv: 'https://arxiv.org/pdf/2206.09203',
      video: 'https://vimeo.com/879078625',
      web: 'https://sites.google.com/view/ivre',
    }
  },
  {
    title: 'DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation',
    authors: 'Hanqing Wang, Wei Liang, Luc Van Gool, Wenguan Wang',
    publisher: 'ICCV 2023 (CCF A)',
    description: 'We propose DREAMWALKER — a world model-based VLN-CE agent that can plan strategically through large amounts of “mental experiments.',
    links: {
      image: '/project/iccv23_dreamwalker/teaser.png',
      paper: 'https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_DREAMWALKER_Mental_Planning_for_Continuous_Vision-Language_Navigation_ICCV_2023_paper.pdf',
      arxiv: 'https://arxiv.org/pdf/2308.07498',
      code: 'https://github.com/hanqingwangai/Dreamwalker',
    }
  },
  {
    title: 'MEWL: Few-shot Multimodal Word Learning with Referential Uncertainty',
    authors: 'Guangyuan Jiang, Manjie Xu, Shiji Xin, Wei Liang, Yujia Peng, Chi Zhang, Yixin Zhu',
    publisher: 'ICML 2023 (CCF A)',
    description: 'We introduce the MachinE Word Learning (MEWL) benchmark to assess how machines learn word meaning in grounded visual scenes.',
    links: {
      image: '/project/icml23_mewl/teaser.png',
      paper: 'https://proceedings.mlr.press/v202/jiang23i/jiang23i.pdf',
      arxiv: 'https://arxiv.org/pdf/2306.00503',
      video: 'https://vimeo.com/843479503',
      code: 'https://github.com/jianggy/MEWL',
      web: 'https://sites.google.com/view/mewl',
    }
  },
  {
    title: 'Discovering the Real Association: Multimodal Causal Reasoning in Video Question Answering',
    authors: 'Chuanqi Zang, Hanqing Wang, Mingtao Pei, Wei Liang',
    publisher: 'CVPR 2023 (CCF A)',
    description: 'Video Question Answering (VideoQA) is challenging as it requires capturing accurate correlations between modalities from redundant information. We discover the real association by explicitly capturing visual features causally related to the question semantics and weakening the impact of local language semantics on question answering.',
    links: {
      image: '/project/cvpr23_dra/teaser.png',
      paper: 'https://openaccess.thecvf.com/content/CVPR2023/papers/Zang_Discovering_the_Real_Association_Multimodal_Causal_Reasoning_in_Video_Question_CVPR_2023_paper.pdf',
    }
  },
  {
    title: 'Diffusion-based Generation, Optimization, and Planning in 3D Scenes',
    authors: 'Siyuan Huang*, Zan Wang*, Puhao Li, Baoxiong Jia, Tengyu Liu, Yixin Zhu, Wei Liang, Song-Chun Zhu',
    publisher: 'CVPR 2023 (CCF A)',
    description: 'We introduce SceneDiffuser, a diffusion-based conditional generative model for 3D scene understanding. SceneDiffuser is applicable to various scene-conditioned 3D tasks.',
    links: {
      image: '/project/cvpr23_scenediffuser/teaser.png',
      paper: 'https://scenediffuser.github.io/paper.pdf',
      arxiv: 'https://arxiv.org/pdf/2301.06015',
      video: 'https://www.youtube.com/watch?v=a0SSprBoVV4',
      code: 'https://github.com/scenediffuser/Scene-Diffuser',
      web: 'https://scenediffuser.github.io/',
    }
  },
  {
    title: 'Optimizing Product Placement for Virtual Stores',
    authors: 'Wei Liang, Luhui Wang, Xinzhe Yu, Changyang Li, Rawan Alghofaili, Yining Lang, Lap-Fai Yu',
    publisher: 'IEEE VR 2023 (CCF A)',
    description: 'We propose a novel approach for automatically optimizing product placement in virtual stores. This approach considers product exposure and spatial constraints and applies an optimizer to search for optimal placement solutions.',
    links: {
      image: '/project/vr23_store/teaser.png',
      paper: '/project/vr23_store/Store_VR2023.pdf',
      video: 'https://youtu.be/J1Pd8ch4eYk',
    }
  },
  {
    title: 'Active Perception for Visual-Language Navigation',
    authors: 'Hanqing Wang, Wenguan Wang, Wei Liang, Steven C. H. Hoi, Jianbing Shen, Luc Van Gool',
    publisher: 'IJCV 2023 (CCF A)',
    description: 'This work draws inspiration from human navigation behavior and endows an agent with an active perception ability for more intelligent navigation. To this end, we propose an end-to-end framework for learning an exploration policy that decides (i) when and where to explore, (ii) what information is worth gathering during exploration, and (iii) how to adjust the navigation decision after the exploration.',
    links: {
      image: '/project/ijcv23_active/teaser.jpg',
      paper: 'https://link.springer.com/article/10.1007/s11263-022-01721-6',
    }
  },
  {
    title: 'Towards Versatile Embodied Navigation',
    authors: 'Hanqing Wang, Wei Liang, Luc Van Gool, and Wenguan Wang.',
    publisher: 'NeurIPS 2022 (CCF A), <font style="color:#FF0800">Spotlight</font>',
    description: 'We introduce VXN, a large multi-task embodied navigation dataset comprising image-goal navigation, object-goal navigation, audio-goal navigation, and vision-language navigation. To solve this challenging task, we introduce Vienna, a versatile navigation agent.',
    links: {
      image: '/project/neurips22_mtn/teaser.png',
      paper: 'https://proceedings.neurips.cc/paper_files/paper/2022/file/ef4f2a0232a246b8a502135175e08953-Paper-Conference.pdf',
      arxiv: 'https://arxiv.org/pdf/2210.16822',
      code: 'https://github.com/HanqingWangAI/VXN',
    }
  },
  {
    title: 'HUMANISE: Language-conditioned Human Motion Generation in 3D Scenes',
    authors: 'Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Wei Liang, and Siyuan Huang.',
    publisher: 'NeurIPS 2022 (CCF A)',
    description: 'We propose a large-scale and semantic-rich human-scene interaction dataset, HUMANISE, which provides language description for each human-scene interaction. HUMANISE enables a new generation task: language-conditioned human motion generation in 3D scenes.',
    links: {
      image: '/project/neurips22_humanise/teaser.png',
      paper: 'https://proceedings.neurips.cc/paper_files/paper/2022/file/6030db5195150ac86d942186f4abdad8-Paper-Conference.pdf',
      arxiv: 'https://arxiv.org/abs/2210.09729',
      video: 'https://www.youtube.com/watch?v=NIdqvN8ex4M',
      code: 'https://github.com/Silverster98/HUMANISE',
      web: 'https://silvester.wang/HUMANISE/'
    }
  },
  {
    title: 'Counterfactual Cycle-Consistent Learning for Instruction Following and Generation in Vision-Language Navigation',
    authors: 'Hanqing Wang, Wei Liang, Jianbing Shen, Luc Van Gool, and Wenguan Wang.',
    publisher: 'CVPR 2022 (CCF A)',
    description: 'We propose a unified learning framework that leverages cycle consistency and counterfactual thinking to jointly learn a navigation agent (follower), an instruction generation agent (speaker), and a counterfactual environment generation agent (creator).',
    links: {
      image: '/project/cvpr22_counterfactual/teaser.png',
      paper: 'https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Counterfactual_Cycle-Consistent_Learning_for_Instruction_Following_and_Generation_in_Vision-Language_CVPR_2022_paper.pdf',
      code: 'https://github.com/HanqingWangAI/CCC-VLN'
    }
  },
  {
    title: 'Structured Scene Memory for Vision-Language Navigation',
    authors: 'Hanqing Wang, Wenguan Wang, Wei Liang, and Jianbing Shen',
    publisher: 'CVPR 2021 (CCF A)',
    description: 'We propose a novel scene representation called Structured Scene Memory (SSM) for vision-language navigation tasks. It explicitly preserves navigation memory and provides global action space for navigation agents.',
    links: {
      image: 'https://i.loli.net/2021/06/27/cLnTK41wvZsXO5C.png',
      paper: 'https://hanqingwangai.github.io/assets/files/posts/ssm/ssm.pdf',
      arxiv: 'https://arxiv.org/abs/2103.03454',
      code: 'https://github.com/HanqingWangAI/SSM-VLN',
    }
  },
  {
    title: 'Scene-Aware Behavior Synthesis for Virtual Pets in Mixed Reality',
    authors: 'Wei Liang, Xinzhe Yu, Rawan Alghofaili, Yining Lang, Lai-Fai Yu',
    publisher: 'ACM CHI 2021 (CCF A)',
    description: 'We propose a novel approach to synthesize virtual pet behaviors by considering scene semantics, enabling a virtual pet to behave naturally in mixed reality.',
    links: {
      image: 'https://i.loli.net/2021/02/07/VgnQBWTcsak15Et.jpg',
      paper: '/project/chi21_pets/chi21_pets.pdf',
      video: 'https://youtu.be/VbAe-0_SNXg',
    }
  },
  {
    title: 'Toward Automatic Audio Description Generation for Accessible Videos',
    authors: 'Yujia Wang, Wei Liang, Haikun Huang, Yongqi Zhang, Dingzeyu Li, and Lap-Fai Yu',
    publisher: 'ACM CHI 2021 (CCF A)',
    description: 'We propose an audio description system that automatically generates audio descriptions for videos. Our user study shows that, with our audio descriptions, blind participants are more confident about what is happening in a video.',
    links: {
      image: 'https://i.loli.net/2021/02/07/wCsZoqf3A6IEuQR.jpg',
      paper: 'https://bitwangyujia.github.io/research/paper/SIGCHI2021-ad.pdf',
      video: 'https://youtu.be/S45vFg476aM',
    }
  },
  {
    title: 'Climaxing VR Character with Scene-Aware Aesthetic Dress Synthesis',
    authors: 'Sifan Hou, Yujia Wang, Wei Liang, and Bing Ning',
    publisher: 'IEEE VR 2021 (CCF A)',
    description: "We propose an approach to automatically synthesize aesthetic dresses for the virtual character within different scenes. The synthesized clothes are in harmony with scene semantics (e.g., season and occasion) and the character's appearance attributes (e.g., gender and age).",
    links: {
      image: 'https://i.loli.net/2021/02/08/OEkoLZwRvVse8tK.jpg',
      paper: '/project/vr21_dress/Dress_VR2021.pdf',
    }
  },
  {
    title: 'Work Surface Arrangement Optimization Driven by Human Activity',
    authors: 'JingJing Liu, Wei Liang, Bing Ning, and Ting Mao',
    publisher: 'IEEE VR 2021 (CCF A)',
    description: 'We propose an approach to capture human habitual behaviors of interacting with objects on the work surface via a Hololens helmet. The habitual behaviors are subsequently applied to optimize the arrangement of the work surface, resulting in a personalized arrangement.',
    links: {
      image: 'https://i.loli.net/2021/02/07/q7gu5zy3mwQsDkX.jpg',
      paper: '/project/vr21_worksurface/Work_Surface_VR2021.pdf',
    }
  },
  {
    title: 'Scene Mover: Automatic Move Planning for Scene Arrangement by Deep Reinforcement Learning',
    authors: 'Hanqing Wang, Wei Liang, and Lap-Fai Yu',
    publisher: 'SIGGRAPH Asia 2020 (CCF A)',
    description: 'We introduce a novel task, scene rearrangement, and an effective solution that leverages MCTS and deep reinforcement learning to tackle it.',
    links: {
      image: 'https://i.loli.net/2021/02/07/hWCi6AT4IvmD3fd.gif',
      paper: 'https://hanqingwangai.github.io/assets/files/posts/mover/mover.pdf',
      code: 'https://github.com/HanqingWangAI/SceneMover',
      video: 'https://youtu.be/zSM-s7zh-vk'
    }
  },
  {
    title: 'Scene-Aware Background Music Synthesis',
    authors: 'Yujia Wang, Wei Liang, Wanwan Li, Dingzeyu Li, and Lap-Fai Yu',
    publisher: 'ACM MM 2020 (CCF A), <font style="color:#FF0800">Oral Presentation (9% acceptance rate) </font>',
    description: "We introduce an interactive background music synthesis algorithm guided by visual content that can synthesize dynamic background music for different scenarios. We also conduct quantitative and qualitative analyses of the synthesized results to validate the approach's efficacy.",
    links: {
      image: 'https://i.loli.net/2021/02/07/K8V6PvYqaQk9XIO.jpg',
      paper: '/project/mm20_music/MM-music.pdf',
      video: 'https://youtu.be/fG2u2QG8ejU',
    }
  },
  {
    title: 'Photo Stand-Out: Photography with Virtual Character',
    authors: 'Yujia Wang, Sifan Hou, Bing Ning, and Wei Liang',
    publisher: 'ACM MM 2020 (CCF A), <font style="color:#FF0800">Oral Presentation (9% acceptance rate) </font>',
    description: "We propose a novel optimization framework to synthesize an aesthetic pose for the virtual character concerning the presented user's pose by optimizing a cost function that guides the rotation of each body joint angle.",
    links: {
      image: '/project/mm20_pose/mm20_pose.jpg',
      paper: '/project/mm20_pose/Pose_MM2020.pdf',
      video: 'https://dl.acm.org/doi/10.1145/3394171.3413957#supplementary-materials',
    }
  },
  {
    title: 'Active Visual Information Gathering for Vision-Language Navigation',
    authors: 'Hanqing Wang, Wenguan Wang, Tianmin Shu, Wei Liang, and Jianbing Shen',
    publisher: 'ECCV 2020 (CCF B)',
    description: 'We propose an end-to-end trainable agent with active exploration ability for the VLN task. The agent can intelligently interact with the environment and actively gather information when faced with ambiguous instructions or unconfident navigation decisions.',
    links: {
      image: 'https://i.loli.net/2021/02/07/9nwZJucr5FblGQT.jpg',
      paper: 'https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670307.pdf',
      arxiv: 'https://arxiv.org/abs/2007.08037',
      code: 'https://github.com/HanqingWangAI/Active_VLN',
    }
  },
  {
    title: 'Comic-Guided Speech Synthesis',
    authors: 'Yujia Wang, Wenguan Wang, Wei Liang and Lap-Fai Yu',
    publisher: 'SIGGRAPH Asia 2019 (CCF A)',
    description: 'We propose an approach to automatically narrate the input comic page by synthesizing realistic speeches for each character. The synthesized speeches carry the identity properties (gender and age) and emotion conditions (e.g., happy, sad, and angry) of the characters inferred from the comic input.',
    links: {
      image: 'https://i.loli.net/2020/09/26/RMbt4PJFCVfDErQ.png',
      paper: 'https://bitwangyujia.github.io/research/paper/siga19-comic.pdf',
      video: 'https://www.youtube.com/watch?v=2cOgWoejbr8&feature=youtu.be',
      results: 'https://bitwangyujia.github.io/research/project/Speech_Results/Speech_Results.html',
      web: 'https://bitwangyujia.github.io/research/project/comic2speech.html'
    }
  },
  {
    title: 'A Deep Coarse-to-Fine Network for Head Pose Estimation from Synthetic Data',
    authors: 'Yujia Wang, Wei Liang, Jianbing Shen, Yunde Jia, and Lap-Fai Yu',
    publisher: 'Pattern Recognition 2019',
    description: 'We propose a deep neural network to estimate head poses using the Coarse-to-Fine strategy. To tackle the problem of insufficient annotated data for training, we design a rendering pipeline to synthesize realistic head images and generate an annotated dataset containing 310k head poses.',
    links: {
      image: 'https://i.loli.net/2020/09/26/Baqm1CbKFkfoT53.gif',
      paper: '/project/pr19_headpose/PR-headpose-2019.pdf',
      video: 'https://youtu.be/30nTWHXURjQ',
    }
  },
  {
    title: 'Virtual Agent Positioning Driven by Scene Semantics in Mixed Reality',
    authors: 'Yining Lang, Wei Liang, and Lap-Fai Yu',
    publisher: 'IEEE VR 2019 (CCF A)',
    description: 'We propose a novel approach using scene semantics to guide the positioning of a virtual agent. This approach comprises a visibility term and a spatial term and achieves a higher user evaluation score than the alternative approaches.',
    links: {
      image: 'https://i.loli.net/2020/09/26/8mMNcpvxXsb9g6j.gif',
      paper: '/project/vr19_agent/AR-v22.pdf',
    }
  },
  {
    title: 'Functional Workspace Optimization via Learning Personal Preferences from Virtual Experiences',
    authors: 'Wei Liang, Jingjing Liu, Yining Lang, Bing Ning and Lap-Fai Yu',
    publisher: 'IEEE TVCG 2019 (CCF A)',
    description: "We propose an approach to learn the personal preferences of using a functional workspace by analyzing a user performing given tasks via VR devices, e.g., making a salad in the kitchen. The learned preferences are applied to optimizing the workspace, which results in an updated layout that fits the user's preferences better.",
    links: {
      image: 'https://i.loli.net/2020/09/26/nbygOF6vptMuPiY.gif',
      paper: '/project/kitchen/tvcg19kitchen.pdf',
      video: 'https://youtu.be/2Eg3_0vHU2U',
    }
  },
  {
    title: 'Deep Single-View 3D Object Reconstruction with Visual Hull Embedding',
    authors: 'Hanqing Wang, Jiaolong Yang, Wei Liang and Xin Tong',
    publisher: 'AAAI 2019 (CCF A), <font style="color:#FF0800">Oral Presentation </font>',
    description: 'We propose a 3D reconstruction approach that preserves more shape details and improves the reconstruction quality. Our key idea is to leverage object mask and pose estimation from CNNs to assist 3D shape learning by constructing a probabilistic single-view visual hull inside the network.',
    links: {
      image: 'https://i.loli.net/2020/09/26/SC1dtHu6JKarlxR.gif',
      paper: 'https://ojs.aaai.org/index.php/AAAI/article/view/4922',
      arxiv: 'https://arxiv.org/pdf/1809.03451.pdf',
      video: 'https://youtu.be/nBjzM7PasGk',
      ppt: '/project/aaai19_3d_recon/presentation.pdf',
    }
  },
  {
    title: '3D Face Synthesis Driven by Personality Impression',
    authors: 'Yining Lang, Wei Liang, Yujia Wang, and Lap-Fai Yu',
    publisher: 'AAAI 2019 (CCF A)',
    description: 'We propose a novel approach to synthesize 3D faces based on personality impressions for creating virtual characters. We demonstrate that our approach can automatically synthesize a variety of 3D faces to give different personality impressions.',
    links: {
      image: 'https://i.loli.net/2020/09/26/lQUSbprea718hmZ.gif',
      paper: 'https://ojs.aaai.org/index.php/AAAI/article/view/3992',
      arxiv: 'https://arxiv.org/pdf/1809.10402',
      video: 'https://youtu.be/B8b8iQOu45c',
    }
  },
  {
    title: 'Tracking Occluded Objects and Recovering Incomplete Trajectories by Reasoning about Containment Relations and Human Actions',
    authors: 'Wei Liang, Yixin Zhu, and Song-Chun Zhu',
    publisher: 'AAAI 2018 (CCF A)',
    description: 'We propose an algorithm to formulate the tracking problem as a network flow representation encoding containment relations and their changes and demonstrating better performance on tracking occluded objects than baseline methods.',
    links: {
      image: 'https://i.loli.net/2020/09/26/KqYvHmXf8adlDwV.gif',
      paper: 'https://cdn.aaai.org/ojs/12222/12222-13-15750-1-2-20201228.pdf',
      video: 'https://youtu.be/Uc8pKLo319I',
    }
  },
  {
    title: 'Synthesizing Personalized Training Programs for Improving Driving Habits via Virtual Reality',
    authors: 'Yining Lang, Wei Liang, Fang Xu, Yibiao Zhao, and Lap-Fai Yu',
    publisher: 'IEEE VR 2018 (CCF A)',
    description: 'We propose an approach first to identify the improper driving habits of a user when he drives in a virtual city. Then, it synthesizes a pertinent training program to help improve the users driving skills based on the discovered improper habits of the user.',
    links: {
      image: 'https://i.loli.net/2020/09/26/mC1lxOeP4zMDTqk.gif',
      paper: '/project/vr18_Driving/final.pdf',
      video: 'https://youtu.be/KEIFE8ZfWgo',
      ppt: '/project/vr18_Driving/IEEE-VR-DRIVING.pdf',
    }
  },
  {
    title: 'Spatially Perturbed Collision Sounds Attenuate Perceived Causality in 3D Launching Events',
    authors: 'Duotun Wang, James Kubricht, Yixin Zhu, Wei Liang, Song-Chun Zhu, Chenfanfu Jiang, and Hongjing Lu',
    publisher: 'IEEE VR 2018 (CCF A)',
    description: 'We discover that people can localize sound positions based on the auditory inputs in VR environments, and the spatial discrepancy between the estimated position of the collision sound and the visually observed impact location attenuates perceived causality.',
    links: {
      image: 'https://i.loli.net/2020/09/26/6WqKGnrNezj7m3P.gif',
      paper: '/project/vr18_causal/vr18b-sub1099-cam-i6.pdf',
      video: 'https://youtu.be/s0pPDoyPnCk',
      ppt: '/project/vr18_causal/ieeevr2018_oral_final _for pdf.pdf',
    }
  },
  {
    title: 'Transferring Object: Joint Inference of Container and Human Pose',
    authors: 'Hanqing Wang, Wei Liang, and Lap-Fai Yu',
    publisher: 'ICCV 2017 (CCF A)',
    description: 'We propose an approach to jointly infer container and human pose for transferring objects by minimizing the costs associated with both object and pose candidates.',
    links: {
      image: 'https://i.loli.net/2020/09/26/hszOl8Aw5u2Z7K6.gif',
      paper: 'https://openaccess.thecvf.com/content_ICCV_2017/papers/Wang_Transferring_Objects_Joint_ICCV_2017_paper.pdf',
      video: 'https://youtu.be/oCNCHbYmGgg',
    }
  },
  {
    title: 'Earthquake Safety Training through Virtual Drills',
    authors: 'Changyang Li, Wei Liang, Chris Quigley, Yibiao Zhao, and Lap-Fai Yu',
    publisher: 'IEEE TVCG 2017 (CCF A)',
    description: 'We provide an immersive and novel virtual reality training approach designed to teach individuals how to survive earthquakes in indoor environments. Our approach utilizes virtual environments realistically populated with furniture objects for training.',
    links: {
      image: 'https://i.loli.net/2020/09/26/KcHQkZiz56eMvmp.gif',
      paper: '/project/tvcg17_earthquake/vr2017earthquake.pdf',
      video: 'https://youtu.be/NdVcUAB0XAE',
    }
  },
  {
    title: 'What is Where: Inferring Containment Relations from Videos',
    authors: 'Wei Liang, Yibiao Zhao, Yixin Zhu, and Song-chun Zhu',
    publisher: 'IJCAI 2016 (CCF A), <font style="color:#FF0800">Oral Presentation </font>',
    description: 'We present a probabilistic approach to explicitly infer containment relations between objects in 3D scenes. Given an input RGB-D video, our algorithm quantizes the perceptual space of a 3D scene by reasoning about containment relations over time.',
    links: {
      image: 'https://i.loli.net/2020/09/26/DesYv2wZ9iBqMTA.gif',
      paper: 'https://www.ijcai.org/Proceedings/16/Papers/483.pdf',
      video: 'https://youtu.be/VQlssiSGQHc',
    }
  },
  {
    title: '3D Head Pose Estimation with Convolutional Neural Network Trained on Synthetic Images',
    authors: 'Xiabing Liu, Wei Liang, Yumeng Wang, Shuyang Li, and Mingtao Pei',
    publisher: 'ICIP 2016',
    description: 'We propose a method to estimate head pose using a convolutional neural network trained on synthetic head images. We formulate head pose estimation as a regression problem and evaluate our method on synthetic and real data.',
    links: {
      image: 'https://i.loli.net/2020/09/26/cZl9IMFHp2iLArs.gif',
      paper: '/project/icip16_headpose/icip16_headpose.pdf',
    }
  },
  {
    title: 'Evaluating Human Cognition of Containing Relations with Physical Simulation',
    authors: 'Wei Liang, Yibiao Zhao, Yixin Zhu, and Songchun Zhu',
    publisher: 'CogSci 2015 (CCF B)',
    description: 'We discover that physical simulation is an excellent approximation of the human cognition of containers and the containing relations using physical simulation. We also analyze human judgments concerning physical simulation results under different scenarios.',
    links: {
      image: 'https://i.loli.net/2020/09/26/843unv9Cjz75m6N.gif',
      paper: 'https://escholarship.org/content/qt1c03t43q/qt1c03t43q_noSplash_a030c30b393739b433e01132c78ddb4f.pdf',
      video: 'https://www.youtube.com/watch?v=kmO4MptwhhM',
      ppt: '/project/cogsci15_container/ppt.pdf',
    }
  }
]
