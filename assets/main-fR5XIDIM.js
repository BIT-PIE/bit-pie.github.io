import{c as b,a as n,o as a,b as d,d as t,t as f,n as M,F as g,r as h,e as I,f as V,g as R,h as W,i as P,j,u as F,k as w,l as H}from"./runtime-dom.esm-bundler-DFvu3hut.js";const T="/assets/logo-zux7KKKu.jpg",Y={key:0,class:"max-w-36 aspect-1/1 overflow-hidden rounded-full mx-auto"},z=["src"],D={key:1,class:"mt-4 font-semibold text-center"},q={key:2,class:"mt-4 font-semibold text-center"},O={key:0},Z=["href"],G={key:1},J={class:"text-sm text-center text-gray-500 dark:text-neutral-400"},B={key:3,class:"mt-2 flex gap-4 justify-center"},X=["href"],$=["href"],Q=["href"],U=5,y={__name:"Avatar",props:{isAlumni:{type:Boolean,default:!1},enName:{type:String,default:""},chName:{type:String,default:""},description:{type:String,default:""},links:{type:Object,default:()=>({})}},setup(r){const l=b(()=>`lg:w-1/${U+1}`),s=b(()=>r.links&&r.links.avatar?r.links.avatar:"https://placehold.co/300x300");return(m,u)=>(a(),n("div",{class:M([l.value,"max-w-72 w-1/1 lg:w-1/6 px-3"])},[r.isAlumni?d("",!0):(a(),n("div",Y,[t("img",{class:"size-full object-cover rounded-full",src:s.value,alt:"Person Image"},null,8,z)])),r.isAlumni?(a(),n("h2",q,[r.links.website?(a(),n("span",O,[t("a",{href:r.links.website,class:"simple-link"},f(r.enName),9,Z)])):(a(),n("span",G,f(r.enName),1))])):(a(),n("h2",D,f(r.enName)+f(r.chName?` (${r.chName})`:""),1)),t("h3",J,f(r.description),1),r.isAlumni?d("",!0):(a(),n("div",B,[r.links.email?(a(),n("a",{key:0,href:`mailto:${r.links.email}`,class:"inline-flex justify-center items-center size-6 rounded-full simple-link"},u[0]||(u[0]=[t("svg",{t:"1755680125904",class:"icon fill-current",viewBox:"0 0 1024 1024",version:"1.1",xmlns:"http://www.w3.org/2000/svg","p-id":"7470"},[t("path",{d:"M926.47619 355.644952V780.190476a73.142857 73.142857 0 0 1-73.142857 73.142857H170.666667a73.142857 73.142857 0 0 1-73.142857-73.142857V355.644952l304.103619 257.828572a170.666667 170.666667 0 0 0 220.745142 0L926.47619 355.644952zM853.333333 170.666667a74.044952 74.044952 0 0 1 26.087619 4.778666 72.704 72.704 0 0 1 30.622477 22.186667 73.508571 73.508571 0 0 1 10.678857 17.67619c3.169524 7.509333 5.12 15.652571 5.607619 24.210286L926.47619 243.809524v24.380952L559.469714 581.241905a73.142857 73.142857 0 0 1-91.306666 2.901333l-3.632762-2.925714L97.52381 268.190476v-24.380952a72.899048 72.899048 0 0 1 40.155428-65.292191A72.97219 72.97219 0 0 1 170.666667 170.666667h682.666666z","p-id":"7471"})],-1)]),8,X)):d("",!0),r.links.github?(a(),n("a",{key:1,href:r.links.github,target:"_blank",class:"inline-flex justify-center items-center size-6 rounded-full simple-link"},u[1]||(u[1]=[t("svg",{t:"1755680210070",class:"icon fill-current",viewBox:"0 0 1024 1024",version:"1.1",xmlns:"http://www.w3.org/2000/svg","p-id":"5467"},[t("path",{d:"M511.6 76.3C264.3 76.2 64 276.4 64 523.5 64 718.9 189.3 885 363.8 946c23.5 5.9 19.9-10.8 19.9-22.2v-77.5c-135.7 15.9-141.2-73.9-150.3-88.9C215 726 171.5 718 184.5 703c30.9-15.9 62.4 4 98.9 57.9 26.4 39.1 77.9 32.5 104 26 5.7-23.5 17.9-44.5 34.7-60.8-140.6-25.2-199.2-111-199.2-213 0-49.5 16.3-95 48.3-131.7-20.4-60.5 1.9-112.3 4.9-120 58.1-5.2 118.5 41.6 123.2 45.3 33-8.9 70.7-13.6 112.9-13.6 42.4 0 80.2 4.9 113.5 13.9 11.3-8.6 67.3-48.8 121.3-43.9 2.9 7.7 24.7 58.3 5.5 118 32.4 36.8 48.9 82.7 48.9 132.3 0 102.2-59 188.1-200 212.9 23.5 23.2 38.1 55.4 38.1 91v112.5c0.8 9 0 17.9 15 17.9 177.1-59.7 304.6-227 304.6-424.1 0-247.2-200.4-447.3-447.5-447.3z","p-id":"5468"})],-1)]),8,$)):d("",!0),r.links.website?(a(),n("a",{key:2,href:r.links.website,target:"_blank",class:"inline-flex justify-center items-center size-6 rounded-full simple-link"},u[2]||(u[2]=[t("svg",{t:"1755680280647",class:"icon fill-current",viewBox:"0 0 1024 1024",version:"1.1",xmlns:"http://www.w3.org/2000/svg","p-id":"9431"},[t("path",{d:"M640 790.528H297.504v-0.096l86.496 0.032v-118.88c0-0.8 0.224-1.536 0.224-2.304 1.28-69.6 57.984-125.888 127.776-125.888s126.496 56.32 127.776 125.888c0 0.768 0.224 1.504 0.224 2.304v118.944z m240.896-427.68L800 309.856V207.168a32 32 0 1 0-64 0v60.8l-206.464-135.328A31.296 31.296 0 0 0 511.424 128a31.168 31.168 0 0 0-17.6 4.64L142.464 362.88a32 32 0 0 0 35.072 53.536L192 406.912V800c0 30.08 27.168 54.592 60.576 54.592h518.848C804.832 854.56 832 830.08 832 800V407.36l13.856 9.056a31.968 31.968 0 0 0 35.04-53.536z","p-id":"9432"})],-1)]),8,Q)):d("",!0)]))],2))}},K={class:"w-4/5 flex flex-col border border-gray-200 border-t-4 border-t-blue-500 shadow-2xs rounded-xl mx-3 lg:mx-6 my-3 dark:border-neutral-700 dark:border-t-blue-500 dark:shadow-neutral-700"},ee={class:"p-4 md:p-5"},te={class:"text-lg font-bold text-gray-800 dark:text-white"},ie=["href"],ae={class:"mt-1 text-sm text-gray-600 dark:text-neutral-400"},ne=["innerHTML"],oe={class:"mt-2 text-gray-500 dark:text-neutral-400"},se={class:"mt-4 flex flex-wrap gap-2"},re=["href"],le={__name:"Publication",props:{title:{type:String,default:""},authors:{type:String,default:""},publisher:{type:String,default:""},links:{type:Object,default:()=>({})}},setup(r){const l=b(()=>{let s={};for(const[m,u]of Object.entries(r.links))m!=="image"&&u&&u.trim()!==""&&u!=="#"&&u!=="/"&&(s[m]=u);return s});return(s,m)=>(a(),n("div",K,[t("div",ee,[t("h3",te,[t("a",{href:r.links.web?r.links.web:r.links.paper,class:"hover:underline"},f(r.title),9,ie)]),t("p",ae,[t("span",{innerHTML:r.publisher},null,8,ne)]),t("p",oe,f(r.authors),1),t("div",se,[(a(!0),n(g,null,h(l.value,(u,v)=>(a(),n("a",{key:v,href:u,target:"_blank",class:"inline-flex items-center gap-x-1.5 py-1.5 px-3 rounded-full text-xs font-medium border border-blue-600 text-blue-600 dark:border-blue-400 dark:text-blue-400"},f(v),9,re))),128))])])]))}};class ce{constructor(l="#navMain",s={}){if(!l)throw new Error("First argument cannot be empty");if(!(typeof l=="string"||l instanceof HTMLElement))throw new TypeError("menu can be either string or an instance of HTMLElement");if(typeof s!="object")throw new TypeError("options can only be of type object");this.menuList=l instanceof HTMLElement?l:document.querySelector(l),this.options={sectionSelector:s.sectionSelector||"section",targetSelector:s.targetSelector||"a",offset:s.offset||0,hrefAttribute:s.hrefAttribute||"href",activeClass:s.activeClass.trim().split(" ")||["active"],onActive:s.onActive||null},this.sections=document.querySelectorAll(this.options.sectionSelector)}onScroll(){const l=this.getCurrentSection(),s=this.getCurrentMenuItem(l);s&&(this.removeCurrentActive({ignore:s}),this.setActive(s,l))}getCurrentSection(){for(let l=0;l<this.sections.length;l++){const s=this.sections[l],m=s.offsetTop,u=m+s.offsetHeight,v=(document.documentElement.scrollTop||document.body.scrollTop)+this.options.offset;if(v>=m&&v<u)return s}}getCurrentMenuItem(l){if(!l)return;const s=l.getAttribute("id");return this.menuList.querySelector(`[${this.options.hrefAttribute}="#${s}"]`)}setActive(l,s){this.options.activeClass.every(u=>l.classList.contains(u))||(l.classList.add(...this.options.activeClass),typeof this.options.onActive=="function"&&this.options.onActive(l,s))}removeCurrentActive(l={ignore:null}){const{hrefAttribute:s,targetSelector:m}=this.options;this.menuList.querySelectorAll(`${m}:not([${s}="${l.ignore.getAttribute(s)}"])`).forEach(v=>v.classList.remove(...this.options.activeClass))}}function pe(r,l={}){const s=new ce(r,l);return window.onload=function(){s.onScroll()},window.addEventListener("scroll",()=>{s.onScroll()}),s}const ue=[{date:"2025.08",content:'Our paper "CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks" was accepted by CoRL 2025. Congratulations to Yixuan!'},{date:"2025.07",content:'Our paper "LiteAT: A Data-Lightweight and User-Adaptive VR Telepresence System for Remote Education" was accepted by IEEE ISMAR 2025. Congratulations to Yuxin!'},{date:"2024.12",content:'Our paper "FloNa: Floor Plan Guided Embodied Visual Navigation" was accepted as an oral presentation at AAAI 2025. Congratulations to Jiaxin and Weiqi!'},{date:"2024.12",content:`Our paper "X's Day: Personality-Driven Virtual Human Behavior Generation" was accepted by TVCG Special Issue on IEEE VR 2025. Congratulations to Haoyang!`},{date:"2024.07",content:"Two papers were accepted by IROS 2024. Congratulations to Jiaxin, Zan, and Hanqing!"},{date:"2024.05",content:"One paper was accepted by ICML 2024. Congratulations to Manjie!"},{date:"2024.02",content:"One paper was accepted by CVPR 2024. Congratulations to Zan!"},{date:"2024.01",content:"One paper was accepted by IEEE VR 2024. Congratulations to Yuxin!"},{date:"2023.07",content:"Two papers were accepted by NeurIPS 2023. Congratulations to Manjie!"},{date:"2023.07",content:'One paper "DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation" was accepted by ICCV 2023. Congratulations to Hanqing!'},{date:"2023.04",content:'One paper "MEWL: Few-shot multimodal word learning with referential uncertainty" was accepted by ICML 2023. Congratulations to Manjie!'},{date:"2023.02",content:"Two papers were accepted by CVPR 2023. Congratulations to Chuanqi, Hanqing, and Zan!"},{date:"2023.01",content:'Our paper "Optimizing Product Placement for Virtual Stores" is accepted by VR 2023. Congratulations to Luhui!'},{date:"2022.09",content:"Two papers were accepted by NeurIPS 2022. Congratulations to Zan and Hanqing!"},{date:"2022.03",content:'Our paper "Counterfactual Cycle-Consistent Learning for Instruction Following and Generation in Vision-Language Navigation" was accepted by CVPR 2022. Congratulations to Hanqing!'},{date:"2021.03",content:'Our paper "Structured Scene Memory for Vision-Language Navigation" was accepted by CVPR 2021. Congratulations to Hanqing!'},{date:"2021.03",content:"Two papers were accepted by ACM CHI 2021. Congratulations to Yujia!"},{date:"2021.02",content:"Two papers were accepted by IEEE VR 2021. Congratulations to Sifan and Jingjing!"},{date:"2020.08",content:'Our paper "Scene Mover: Automatic Move Planning for Scene Arrangement by Deep Reinforcement Learning" was accepted by SIGGRAPH Asia 2020. Congratulations to Hanqing!'},{date:"2020.07",content:"Two papers were accepted by ACM MM 2020. Congratulations to Yujia and Sifan!"},{date:"2019.08",content:'Our paper "Comic-Guided Speech Synthesis" was accepted by SIGGRAPH Asia 2019. Congratulations to Yujia!'},{date:"2019.05",content:'Our paper "A Deep Coarse-to-Fine Network for Head Pose Estimation from Synthetic Data" was accepted by Pattern Recognition. Congratulations to Yujia!'},{date:"2019.02",content:'Our paper "Functional Workspace Optimization via Learning Personal Preferences from Virtual Experiences" was reported by <a href="https://shiropen.com/seamless/functional-workspace-optimization-via-learning-personal-preferences-from-virtual-experiences" class="simple-link">Seamless</a>. Congratulations to Jingjing!'},{date:"2019.02",content:'Our paper "Virtual Agent Positioning Driven by Scene Semantics in Mixed Reality" was reported by <a href="https://shiropen.com/seamless-virtual-agent-positioning-driven" class="simple-link">Seamless</a>. Congratulations to Yining!'},{date:"2019.02",content:"Our paper was accepted by IEEE Virtual Reality 2019. Congratulations to Yining!"},{date:"2018.11",content:"Our paper was accepted by TVCG Special Issue on IEEE Virtual Reality and 3D User Interfaces (IEEE VR) 2019. Congratulations to Jingjing!"},{date:"2018.11",content:"Two papers were accepted by AAAI 2019. Congratulations to Hanqing and Yining!"},{date:"2018.10",content:"Yining received the 2018 national fellowship. Congratulations!"},{date:"2018.01",content:'Our paper "Synthesizing Personalized Training Programs for Improving Driving Habits via Virtual Reality" has been accepted by IEEE VR 2018.'},{date:"2018.01",content:'Our paper "Spatially Perturbed Collision Sounds Attenuate Perceived Causality in 3D Launching Events" has been accepted by IEEE VR 2018.'},{date:"2017.11",content:'Our paper "Tracking Occluded Objects and Recovering Incomplete Trajectories by Reasoning about Containment Relations and Human Actions" has been accepted to AAAI 2018.'},{date:"2017.07",content:'Our paper "Transferring Object: Joint Inference of Container and Human Pose" has been accepted to ICCV 2017.'}],e=Object.freeze({faculty:"Faculty",student:"Student",undergraduate:"Undergraduate",alumni:"Alumni"}),k=[{enName:"Wei Liang",chName:"梁玮",role:e.faculty,description:"Professor",links:{avatar:"/images/people/liangwei.png",website:"https://liangwei-bit.github.io/web/"}},{enName:"Hao Xu",chName:"徐浩",role:e.student,description:"Master Student ’25",links:{avatar:"/images/people/xuhao.jpg"}},{enName:"Qiuyu Li",chName:"李秋予",role:e.student,description:"Master Student ’25",links:{avatar:"/images/people/liqiuyu.jpg"}},{enName:"Minghe Zhang",chName:"张明贺",role:e.student,description:"Master Student ’25",links:{avatar:"/images/people/zhangminghe.jpg"}},{enName:"Wen Li",chName:"李稳",role:e.student,description:"Master Student ’25",links:{avatar:"/images/people/liwen.jpg"}},{enName:"Zan Wang",chName:"王赞",role:e.student,description:"PhD Student ’21",links:{avatar:"/images/people/wangzan.jpg",email:"zanwang98@gmail.com",github:"https://github.com/Silverster98",website:"https://silvester.wang"}},{enName:"Jiaxin Li",chName:"李佳鑫",role:e.student,description:"PhD Student ’23",links:{avatar:"/images/people/lijiaxin.jpg",email:"GualeeJX@outlook.com",github:"https://github.com/GauleeJX",website:"https://gauleejx.github.io/"}},{enName:"Yixuan Li",chName:"李逸旋",role:e.student,description:"PhD Student ’23",links:{avatar:"/images/people/liyixuan.jpg",email:"liyixxuan@gmail.com",github:"https://github.com/yixxuan-li",website:"https://yixxuan-li.github.io/"}},{enName:"Xiaozhi Li",chName:"李晓之",role:e.student,description:"PhD Student ’23",links:{avatar:"/images/people/lixiaozhi.jpg",website:"https://lixzzzzzz.github.io/"}},{enName:"Ruiqi Cheng",chName:"成睿琦",role:e.student,description:"PhD Student ’23",links:{avatar:"/images/people/chengruiqi.jpg",website:"https://reachel1.github.io/"}},{enName:"Yuxin Shen",chName:"沈雨欣",role:e.student,description:"PhD Student ’24",links:{avatar:"/images/people/shenyuxin.jpg",github:"https://github.com/SYXcandice"}},{enName:"Weiqi Huang",chName:"黄尉琪",role:e.student,description:"PhD Student ’24",links:{avatar:"/images/people/huangweiqi.jpg",website:"https://wikiahuang.github.io/"}},{enName:"Xinfeng Gao",chName:"高鑫峰",role:e.student,description:"Master Student ’23",links:{avatar:"/images/people/gaoxinfeng.jpg"}},{enName:"Lebin Ding",chName:"丁乐斌",role:e.student,description:"Master Student ’23",links:{avatar:"/images/people/dinglebin.jpg"}},{enName:"Zeqian Li",chName:"李泽乾",role:e.student,description:"Master Student ’23",links:{avatar:"/images/people/lizeqian.jpg"}},{enName:"Luya Mo",chName:"莫璐雅",role:e.student,description:"Master Student ’23",links:{avatar:"/images/people/moluya.jpg"}},{enName:"Yu Zong",chName:"宗宇",role:e.student,description:"Master Student ’23",links:{avatar:"/images/people/zongyu.jpg"}},{enName:"Zhipeng Lou",chName:"娄志鹏",role:e.student,description:"Master Student ’23",links:{avatar:"/images/people/louzhipeng.jpg"}},{enName:"Haoyang Li",chName:"李昊阳",role:e.student,description:"Master Student ’23",links:{avatar:"/images/people/lihaoyang.jpg",website:"https://lihaoyang.netlify.app/"}},{enName:"Yili Wang",chName:"王宜立",role:e.student,description:"Master Student ’23",links:{avatar:"/images/people/wangyili.jpg"}},{enName:"Zimo Zhai",chName:"翟子墨",role:e.student,description:"Master Student ’24",links:{avatar:"/images/people/zhaizimo.jpg"}},{enName:"Canzhi Chen",chName:"陈灿之",role:e.student,description:"Master Student ’24",links:{avatar:"/images/people/chencanzhi.jpg"}},{enName:"Yue Zhang",chName:"张越",role:e.student,description:"Master Student ’24",links:{avatar:"/images/people/zhangyue.jpg"}},{enName:"Yifei Guo",chName:"郭艺飞",role:e.student,description:"Master Student ’24",links:{avatar:"/images/people/guoyifei.jpg"}},{enName:"Junfu Li",chName:"李俊甫",role:e.student,description:"Master Student ’24",links:{avatar:"/images/people/lijunfu.jpg"}},{enName:"Qi Wu",chName:"吴琦",role:e.student,description:"Master Student ’24",links:{avatar:"/images/people/wuqi.jpg"}},{enName:"Siqi Zhu",chName:"朱思齐",role:e.student,description:"Master Student ’25",links:{avatar:"/images/people/zhusiqi.jpg"}},{enName:"Yujia Wang",chName:"王雨佳",role:e.alumni,description:"PhD Student ’17",links:{avatar:"",website:"https://bitwangyujia.github.io/research/"}},{enName:"Hanqing Wang",chName:"汪汉青",role:e.alumni,description:"PhD Student ’18",links:{avatar:"",website:"https://hanqingwangai.github.io"}},{enName:"Anqi Li",chName:"李安琪",role:e.alumni,description:"Master Student ’22",links:{avatar:""}},{enName:"Jiawen Liang",chName:"梁佳雯",role:e.alumni,description:"Master Student ’22",links:{avatar:""}},{enName:"Qingyun Deng",chName:"邓青云",role:e.alumni,description:"Master Student ’22",links:{avatar:""}},{enName:"Manjie Xu",chName:"徐满杰",role:e.alumni,description:"Master Student ’22",links:{avatar:"",website:"https://ariesssxu.github.io/"}},{enName:"Zhaochen Pang",chName:"庞昭辰",role:e.alumni,description:"Master Student ’22",links:{avatar:""}},{enName:"Dian Rui Chia",chName:"",role:e.alumni,description:"Master Student ’22",links:{avatar:""}},{enName:"Yifan Huang",chName:"黄一帆",role:e.alumni,description:"Master Student ’21",links:{avatar:""}},{enName:"Jiaqi Liu",chName:"刘佳琪",role:e.alumni,description:"Master Student ’21",links:{avatar:""}},{enName:"Jingpin Li",chName:"李景品",role:e.alumni,description:"Master Student ’21",links:{avatar:""}},{enName:"Haoliang Guan",chName:"管浩良",role:e.alumni,description:"Master Student ’21",links:{avatar:""}},{enName:"Luhui Wang",chName:"王潞辉",role:e.alumni,description:"Master Student ’20",links:{avatar:""}},{enName:"Hao Chen",chName:"陈昊",role:e.alumni,description:"Master Student ’20",links:{avatar:""}},{enName:"Wenxuan Zhao",chName:"赵文轩",role:e.alumni,description:"Master Student ’20",links:{avatar:""}},{enName:"Lu Zhao",chName:"赵璐",role:e.alumni,description:"Master Student ’19",links:{avatar:""}},{enName:"Shihao Song",chName:"宋世豪",role:e.alumni,description:"Master Student ’19",links:{avatar:""}},{enName:"Sifan Hou",chName:"侯思凡",role:e.alumni,description:"Master Student ’19",links:{avatar:"",website:"https://bitcynthia.github.io/homepage/index.html"}},{enName:"Zhenyun Lei",chName:"雷镇远",role:e.alumni,description:"Master Student ’19",links:{avatar:""}},{enName:"Jingjing Liu",chName:"刘晶晶",role:e.alumni,description:"Master Student ’18",links:{avatar:""}},{enName:"Xinzhe Yu",chName:"于馨喆",role:e.alumni,description:"Master Student ’18",links:{avatar:""}},{enName:"Honfei Yu",chName:"俞鸿飞",role:e.alumni,description:"Master Student ’18",links:{avatar:""}},{enName:"Yuke Zhang",chName:"张宇柯",role:e.alumni,description:"Master Student ’17",links:{avatar:""}},{enName:"Yining Lang",chName:"郎一宁",role:e.alumni,description:"Master Student ’17",links:{avatar:""}},{enName:"Ting Mao",chName:"毛婷",role:e.alumni,description:"Master Student ’17",links:{avatar:""}},{enName:"Wenqiao Li",chName:"李文乔",role:e.alumni,description:"Master Student ’16",links:{avatar:""}},{enName:"Mengyao Jia",chName:"贾梦瑶",role:e.alumni,description:"Master Student ’15",links:{avatar:""}},{enName:"Yumeng Wang",chName:"王雨濛",role:e.alumni,description:"Master Student ’15",links:{avatar:""}},{enName:"Shuyang Li",chName:"李书洋",role:e.alumni,description:"Master Student ’14",links:{avatar:""}},{enName:"Xiabing Liu",chName:"刘夏冰",role:e.alumni,description:"Master Student ’14",links:{avatar:""}},{enName:"Qun Zhang",chName:"张群",role:e.alumni,description:"Master Student ’13",links:{avatar:""}},{enName:"Yan Liang",chName:"梁燕",role:e.alumni,description:"Master Student ’12",links:{avatar:""}},{enName:"Bingjie Wang",chName:"王冰洁",role:e.alumni,description:"Master Student ’11",links:{avatar:""}},{enName:"Xiaojing Lin",chName:"林晓静",role:e.alumni,description:"Master Student ’10",links:{avatar:""}}],de=[{title:"CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks",authors:"Yixuan Li*, Yutang Lin*, Jieming Cui, Tengyu Liu, Wei Liang, Yixin Zhu, Siyuan Huang",publisher:"CoRL 2025",description:"",links:{image:"",arxiv:"https://arxiv.org/abs/2506.08931",video:"https://vimeo.com/1092360484",code:"https://github.com/humanoid-clone/CLONE/",web:"https://humanoid-clone.github.io/"}},{title:"LiteAT: A Data-Lightweight and User-Adaptive VR Telepresence System for Remote Education",authors:"Yuxin Shen , Wei Liang , Jianzhu Ma",publisher:"TVCG 2025 (Special Issue on IEEE ISMAR 2025) (CCF A)",description:"This paper introduces LiteAT, a data-lightweight and user-adaptive VR telepresence system, to enable real-time, immersive learning experiences.",links:{image:"",web:"https://sites.google.com/view/lite-adaptive-telepresence"}},{title:"Env-Mani: Quadrupedal Robot Loco-Manipulation with Environment-in-the-Loop",authors:"Yixuan Li, Zan Wang, Wei Liang",publisher:"IROS 2025 (CCF C)",description:"",links:{image:"",video:"https://www.youtube.com/watch?v=8BAaLCI8cis",web:"https://sites.google.com/view/env-mani"}},{title:"R2G: Reasoning to Ground in 3D Scenes",authors:"Yixuan Li, Zan Wang, Wei Liang",publisher:"Pattern Recognition 2025 (CCF B)",description:"",links:{image:"",paper:"https://www.sciencedirect.com/science/article/abs/pii/S0031320325003887",arxiv:"arxiv: https://arxiv.org/abs/2408.13499",video:"video: https://www.youtube.com/watch?v=Y2dhg3cyiqY",code:"https://github.com/yixxuan-li/R2G",web:"https://sites.google.com/view/reasoning-to-ground"}},{title:"FloNa: Floor Plan Guided Embodied Visual Navigation",authors:"Jiaxin Li, Weiqi Huang, Zan Wang, Wei Liang, Huijun Di, Feng Liu",publisher:'AAAI 2025 (CCF A), <font style="color:#FF0800">Oral</font>',description:"We propose FloNa, a novel navigation task that leverages prior floor plans for guidance. To efficiently solve FloNa, we introduce FloDiff, a diffusion-based policy framework equipped with a localization module.",links:{image:"./project/aaai25_flona/teaser.jpg",paper:"https://ojs.aaai.org/index.php/AAAI/article/view/33601",arxiv:"https://arxiv.org/abs/2412.18335",video:"https://gauleejx.github.io/flona/",code:"https://github.com/GauleeJX/flodiff",ppt:"/",web:"https://gauleejx.github.io/flona/"}},{title:"X's Day: Personality-Driven Virtual Human Behavior Generation",authors:"Haoyang Li, Zan Wang, Wei Liang, Yizhuo Wang",publisher:"IEEE TVCG (Special Issue on IEEE VR 2025) (CCF A)",description:"This paper introduces a novel task focused on autoregressively generating long-term behaviors for virtual agents, guided by specific personality traits and contextual elements within 3D environments.",links:{image:"./project/vr25_behavior/teaser.jpg",paper:"https://behavior.agent-x.cn/static/files/paper.pdf",arxiv:"",video:"https://www.youtube.com/watch?v=Me8Pd3kAxmM",code:"",ppt:"/",web:"https://behavior.agent-x.cn/"}},{title:"Visual Loop Closure Detection with Thorough Temporal and Spatial Context Exploitation",authors:"Jiaxin Li*, Zan Wang*, Huijun Di, Jian Li, Wei Liang",publisher:'IROS 2024 (CCF C), <font style="color:#FF0800">Oral Pitch</font>',description:"We propose to leverage the global temporal and local spatial-temporal information for loop closure detection.",links:{image:"./project/iros24_tosa/teaser.png",paper:"https://gauleejx.github.io/IROS2024_TOSA/static/pdfs/TOSA.pdf",arxiv:"",video:"https://www.youtube.com/watch?v=CmxsxJgDCXo",code:"",ppt:"/",web:"https://gauleejx.github.io/IROS2024_TOSA/"}},{title:"Mastering Scene Rearrangement with Expert-assisted Curriculum Learning and Adaptive Trade-Off Tree-Search",authors:"Zan Wang*, Hanqing Wang*, Wei Liang",publisher:'IROS 2024 (CCF C), <font style="color:#FF0800">Oral Pitch</font>',description:"We solve scene rearrangement planning by introducing an expert-assisted curriculum learning paradigm and a tree-search-based planner enhanced by an adaptive trade-off strategy.",links:{image:"./project/iros24_plato/teaser.png",paper:"https://pl-ato.github.io/static/pdfs/paper.pdf",arxiv:"",video:"https://www.youtube.com/watch?v=gUYR1Hk8eVI",code:"",ppt:"/",web:"https://pl-ato.github.io/"}},{title:"Prompt-guided Precise Audio Editing with Diffusion Models",authors:"Manjie Xu, Chenxing Li, Dan Su, Wei Liang, Dong Yu",publisher:"ICML 2024 (CCF A)",description:"We present a novel editing approach guided by textual prompt which serves as a general module for diffusion models and enables precise editing of multimodal generation content.",links:{image:"./project/icml24_ppae/teaser.jpg",paper:"https://openreview.net/pdf?id=kQ1dwuheR0",arxiv:"",video:"/",code:"",ppt:"/",web:"https://sites.google.com/view/icml24-ppae"}},{title:"Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance",authors:"Zan Wang, Yixin Chen, Baoxiong Jia, Puhao Li, Jinlu Zhang, Jingze Zhang, Tengyu Liu, Yixin Zhu, Wei Liang, Siyuan Huang",publisher:'CVPR 2024 (CCF A), <font style="color:#FF0800">Highlight</font>',description:"We introduce a novel two-stage framework that employs scene affordance as an intermediate representation, effectively linking 3D scene grounding and conditional motion generation.",links:{image:"./project/cvpr24_afford/teaser.gif",paper:"",arxiv:"https://arxiv.org/abs/2403.18036",video:"https://www.youtube.com/watch?v=emT0FHDYY1U",code:"",ppt:"/",web:"https://afford-motion.github.io/"}},{title:"Context-Aware Head-and-Eye Motion Generation with Diffusion Model",authors:"Yuxin Shen, Manjie Xu, Wei Liang",publisher:"IEEE VR 2024 (CCF A)",description:"We use a diffusion model to generate realistic head-and-eye motions, which are harmoniously aliged with simulated environments.",links:{image:"./project/vr24_eye/teaser.jpg",paper:"./project/vr24_eye/VR2024_Eye.pdf",arxiv:"",video:"https://www.youtube.com/watch?v=_Hf872Ce0v4",code:"",ppt:"/",web:"https://sites.google.com/view/context-aware-generation"}},{title:"Active Reasoning in an Open-World Environment",authors:"Manjie Xu, Guangyuan Jiang, Wei Liang, Chi Zhang, Yixin Zhu",publisher:"NeurIPS 2023 (CCF A)",description:"We build artificial agents that possess the ability to actively explore, accumulate, and reason using both newfound and existing information to tackle incomplete-information questions, functioning like AI detectives.",links:{image:"./project/neurips23_conan/teaser.jpg",paper:"",arxiv:"https://arxiv.org/pdf/2311.02018.pdf",video:"https://vimeo.com/878540519",code:"",ppt:"/",web:"https://sites.google.com/view/conan-active-reasoning"}},{title:"Interactive Visual Reasoning under Uncertainty",authors:"Manjie Xu*, Guangyuan Jiang*, Wei Liang, Chi Zhang, Yixin Zhu",publisher:"NeurIPS 2023 (CCF A)",description:"We test modern artificial agents whether they can quickly resolve uncertainty by generating hypotheses and testing them via active trials when encountering a novel phenomenon accompanied by ambiguous cause-effect relations.",links:{image:"./project/neurips23_ivre/teaser.jpg",paper:"",arxiv:"https://arxiv.org/pdf/2206.09203.pdf",video:"https://vimeo.com/879078625",code:"",ppt:"/",web:"https://sites.google.com/view/ivre"}},{title:"DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation",authors:"Hanqing Wang, Wei Liang, Luc Van Gool, Wenguan Wang",publisher:"ICCV 2023 (CCF A)",description:"We propose DREAMWALKER — a world model-based VLN-CE agent that can plan strategically through large amounts of “mental experiments.",links:{image:"./project/iccv23_dreamwalker/teaser.png",paper:"./project/iccv23_dreamwalker/dreamwalker.pdf",arxiv:"",video:"/",code:"",ppt:"/",web:"https://github.com/hanqingwangai/Dreamwalker"}},{title:"MEWL: Few-shot Multimodal Word Learning with Referential Uncertainty",authors:"Guangyuan Jiang, Manjie Xu, Shiji Xin, Wei Liang, Yujia Peng, Chi Zhang, Yixin Zhu",publisher:"ICML 2023 (CCF A)",description:"We introduce the MachinE Word Learning (MEWL) benchmark to assess how machines learn word meaning in grounded visual scenes.",links:{image:"./project/icml23_mewl/teaser.png",paper:"",arxiv:"https://arxiv.org/abs/2306.00503",video:"https://vimeo.com/843479503",code:"",ppt:"/",web:"https://sites.google.com/view/mewl"}},{title:"Discovering the Real Association: Multimodal Causal Reasoning in Video Question Answering",authors:"Chuanqi Zang, Hanqing Wang, Mingtao Pei, Wei Liang",publisher:"CVPR 2023 (CCF A)",description:"Video Question Answering (VideoQA) is challenging as it requires capturing accurate correlations between modalities from redundant information. We discover the real association by explicitly capturing visual features causally related to the question semantics and weakening the impact of local language semantics on question answering.",links:{image:"./project/cvpr23_dra/teaser.png",paper:"https://openaccess.thecvf.com/content/CVPR2023/papers/Zang_Discovering_the_Real_Association_Multimodal_Causal_Reasoning_in_Video_Question_CVPR_2023_paper.pdf",arxiv:"",video:"/",code:"",ppt:"/",web:"/"}},{title:"Diffusion-based Generation, Optimization, and Planning in 3D Scenes",authors:"Siyuan Huang*, Zan Wang*, Puhao Li, Baoxiong Jia, Tengyu Liu, Yixin Zhu, Wei Liang, Song-Chun Zhu",publisher:"CVPR 2023 (CCF A)",description:"We introduce SceneDiffuser, a diffusion-based conditional generative model for 3D scene understanding. SceneDiffuser is applicable to various scene-conditioned 3D tasks.",links:{image:"./project/cvpr23_scenediffuser/teaser.png",paper:"https://scenediffuser.github.io/paper.pdf",arxiv:"",video:"https://www.youtube.com/watch?v=a0SSprBoVV4",code:"",ppt:"/",web:"https://scenediffuser.github.io/"}},{title:"Optimizing Product Placement for Virtual Stores",authors:"Wei Liang, Luhui Wang, Xinzhe Yu, Changyang Li, Rawan Alghofaili, Yining Lang, Lap-Fai Yu",publisher:"IEEE VR 2023 (CCF A)",description:"We propose a novel approach for automatically optimizing product placement in virtual stores. This approach considers product exposure and spatial constraints and applies an optimizer to search for optimal placement solutions.",links:{image:"./project/vr23_store/teaser.png",paper:"./project/vr23_store/Store_VR2023.pdf",arxiv:"",video:"/",code:"",ppt:"/",web:"/"}},{title:"Active Perception for Visual-Language Navigation",authors:"Hanqing Wang, Wenguan Wang, Wei Liang, Steven C. H. Hoi, Jianbing Shen, Luc Van Gool",publisher:"IJCV 2023 (CCF A)",description:"This work draws inspiration from human navigation behavior and endows an agent with an active perception ability for more intelligent navigation. To this end, we propose an end-to-end framework for learning an exploration policy that decides (i) when and where to explore, (ii) what information is worth gathering during exploration, and (iii) how to adjust the navigation decision after the exploration.",links:{image:"./project/ijcv23_active/teaser.jpg",paper:"https://link.springer.com/article/10.1007/s11263-022-01721-6",arxiv:"",video:"/",code:"",ppt:"/",web:"/"}},{title:"Towards Versatile Embodied Navigation",authors:"Hanqing Wang, Wei Liang, Luc Van Gool, and Wenguan Wang.",publisher:'NeurIPS 2022 (CCF A), <font style="color:#FF0800">Spotlight</font>',description:"We introduce VXN, a large multi-task embodied navigation dataset comprising image-goal navigation, object-goal navigation, audio-goal navigation, and vision-language navigation. To solve this challenging task, we introduce Vienna, a versatile navigation agent.",links:{image:"./project/neurips22_mtn/teaser.png",paper:"./project/neurips22_mtn/MTN.pdf",arxiv:"",video:"/",code:"",ppt:"/",web:"https://github.com/HanqingWangAI/VXN"}},{title:"HUMANISE: Language-conditioned Human Motion Generation in 3D Scenes",authors:"Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Wei Liang, and Siyuan Huang.",publisher:"NeurIPS 2022 (CCF A)",description:"We propose a large-scale and semantic-rich human-scene interaction dataset, HUMANISE, which provides language description for each human-scene interaction. HUMANISE enables a new generation task: language-conditioned human motion generation in 3D scenes.",links:{image:"./project/neurips22_humanise/teaser.png",paper:"",arxiv:"https://arxiv.org/abs/2210.09729",video:"https://www.youtube.com/watch?v=NIdqvN8ex4M",code:"",ppt:"https://recorder-v3.slideslive.com/#/share?share=74179&s=0a18d15c-4ca4-424a-8aa7-a91a7ac3701c",web:"https://silvester.wang/HUMANISE/"}},{title:"Counterfactual Cycle-Consistent Learning for Instruction Following and Generation in Vision-Language Navigation",authors:"Hanqing Wang, Wei Liang, Jianbing Shen, Luc Van Gool, and Wenguan Wang.",publisher:"CVPR 2022 (CCF A)",description:"We propose a unified learning framework that leverages cycle consistency and counterfactual thinking to jointly learn a navigation agent (follower), an instruction generation agent (speaker), and a counterfactual environment generation agent (creator).",links:{image:"./project/cvpr22_counterfactual/teaser.png",paper:"https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Counterfactual_Cycle-Consistent_Learning_for_Instruction_Following_and_Generation_in_Vision-Language_CVPR_2022_paper.pdf",arxiv:"",video:"/",code:"",ppt:"/",web:"https://github.com/HanqingWangAI/CCC-VLN"}},{title:"Structured Scene Memory for Vision-Language Navigation",authors:"Hanqing Wang, Wenguan Wang, Wei Liang, and Jianbing Shen",publisher:"CVPR 2021 (CCF A)",description:"We propose a novel scene representation called Structured Scene Memory (SSM) for vision-language navigation tasks. It explicitly preserves navigation memory and provides global action space for navigation agents.",links:{image:"https://i.loli.net/2021/06/27/cLnTK41wvZsXO5C.png",paper:"https://hanqingwangai.github.io/assets/files/posts/ssm/ssm.pdf",arxiv:"",video:"/",code:"",ppt:"/",web:"https://github.com/HanqingWangAI/SSM-VLN"}},{title:"Scene-Aware Behavior Synthesis for Virtual Pets in Mixed Reality",authors:"Wei Liang, Xinzhe Yu, Rawan Alghofaili, Yining Lang, Lai-Fai Yu",publisher:"ACM CHI 2021 (CCF A)",description:"We propose a novel approach to synthesize virtual pet behaviors by considering scene semantics, enabling a virtual pet to behave naturally in mixed reality.",links:{image:"https://i.loli.net/2021/02/07/VgnQBWTcsak15Et.jpg",paper:"http://liangwei-bit.github.io/web/project/chi21_pets/chi21_pets.pdf",arxiv:"",video:"https://youtu.be/VbAe-0_SNXg",code:"",ppt:"/",web:"http://liangwei-bit.github.io/web/project/chi21_pets"}},{title:"Toward Automatic Audio Description Generation for Accessible Videos",authors:"Yujia Wang, Wei Liang, Haikun Huang, Yongqi Zhang, Dingzeyu Li, and Lap-Fai Yu",publisher:"ACM CHI 2021 (CCF A)",description:"We propose an audio description system that automatically generates audio descriptions for videos. Our user study shows that, with our audio descriptions, blind participants are more confident about what is happening in a video.",links:{image:"https://i.loli.net/2021/02/07/wCsZoqf3A6IEuQR.jpg",paper:"https://bitwangyujia.github.io/research/paper/SIGCHI2021-ad.pdf",arxiv:"",video:"https://youtu.be/S45vFg476aM",code:"",ppt:"/",web:"http://liangwei-bit.github.io/web/project/chi21_description"}},{title:"Climaxing VR Character with Scene-Aware Aesthetic Dress Synthesis",authors:"Sifan Hou, Yujia Wang, Wei Liang, and Bing Ning",publisher:"IEEE VR 2021 (CCF A)",description:"We propose an approach to automatically synthesize aesthetic dresses for the virtual character within different scenes. The synthesized clothes are in harmony with scene semantics (e.g., season and occasion) and the character's appearance attributes (e.g., gender and age).",links:{image:"https://i.loli.net/2021/02/08/OEkoLZwRvVse8tK.jpg",paper:"./project/vr21_dress/Dress_VR2021.pdf",arxiv:"",video:"/",code:"",ppt:"/",web:"/"}},{title:"Work Surface Arrangement Optimization Driven by Human Activity",authors:"JingJing Liu, Wei Liang, Bing Ning, and Ting Mao",publisher:"IEEE VR 2021 (CCF A)",description:"We propose an approach to capture human habitual behaviors of interacting with objects on the work surface via a Hololens helmet. The habitual behaviors are subsequently applied to optimize the arrangement of the work surface, resulting in a personalized arrangement.",links:{image:"https://i.loli.net/2021/02/07/q7gu5zy3mwQsDkX.jpg",paper:"./project/vr21_worksurface/Work_Surface_VR2021.pdf",arxiv:"",video:"/",code:"",ppt:"/",web:"/"}},{title:"Scene Mover: Automatic Move Planning for Scene Arrangement by Deep Reinforcement Learning",authors:"Hanqing Wang, Wei Liang, and Lap-Fai Yu",publisher:"SIGGRAPH Asia 2020 (CCF A)",description:"We introduce a novel task, scene rearrangement, and an effective solution that leverages MCTS and deep reinforcement learning to tackle it.",links:{image:"https://i.loli.net/2021/02/07/hWCi6AT4IvmD3fd.gif",paper:"https://dl.acm.org/doi/abs/10.1145/3414685.3417788",arxiv:"",video:"/",code:"",ppt:"/",web:"https://github.com/HanqingWangAI/SceneMover"}},{title:"Scene-Aware Background Music Synthesis",authors:"Yujia Wang, Wei Liang, Wanwan Li, Dingzeyu Li, and Lap-Fai Yu",publisher:'ACM MM 2020 (CCF A), <font style="color:#FF0800">Oral Presentation (9% acceptance rate) </font>',description:"We introduce an interactive background music synthesis algorithm guided by visual content that can synthesize dynamic background music for different scenarios. We also conduct quantitative and qualitative analyses of the synthesized results to validate the approach's efficacy.",links:{image:"https://i.loli.net/2021/02/07/K8V6PvYqaQk9XIO.jpg",paper:"http://liangwei-bit.github.io/web/project/mm20_music/MM-music.pdf",arxiv:"",video:"https://youtu.be/fG2u2QG8ejU",code:"",ppt:"/",web:"http://liangwei-bit.github.io/web/project/mm20_music"}},{title:"Photo Stand-Out: Photography with Virtual Character",authors:"Yujia Wang, Sifan Hou, Bing Ning, and Wei Liang",publisher:'ACM MM 2020 (CCF A), <font style="color:#FF0800">Oral Presentation (9% acceptance rate) </font>',description:"We propose a novel optimization framework to synthesize an aesthetic pose for the virtual character concerning the presented user's pose by optimizing a cost function that guides the rotation of each body joint angle.",links:{image:"./project/mm20_pose/mm20_pose.jpg",paper:"./project/mm20_pose/Pose_MM2020.pdf",arxiv:"",video:"/",code:"",ppt:"/",web:"/"}},{title:"Active Visual Information Gathering for Vision-Language Navigation",authors:"Hanqing Wang, Wenguan Wang, Tianmin Shu, Wei Liang, and Jianbing Shen",publisher:"ECCV 2020 (CCF B)",description:"We propose an end-to-end trainable agent with active exploration ability for the VLN task. The agent can intelligently interact with the environment and actively gather information when faced with ambiguous instructions or unconfident navigation decisions.",links:{image:"https://i.loli.net/2021/02/07/9nwZJucr5FblGQT.jpg",paper:"",arxiv:"https://arxiv.org/abs/2007.08037",video:"/",code:"",ppt:"/",web:"https://github.com/HanqingWangAI/Active_VLN"}},{title:"Comic-Guided Speech Synthesis",authors:"Yujia Wang, Wenguan Wang, Wei Liang and Lap-Fai Yu",publisher:"SIGGRAPH Asia 2019 (CCF A)",description:"We propose an approach to automatically narrate the input comic page by synthesizing realistic speeches for each character. The synthesized speeches carry the identity properties (gender and age) and emotion conditions (e.g., happy, sad, and angry) of the characters inferred from the comic input.",links:{image:"https://i.loli.net/2020/09/26/RMbt4PJFCVfDErQ.png",paper:"https://bitwangyujia.github.io/research/paper/siga19-comic.pdf",arxiv:"",video:"https://www.youtube.com/watch?v=2cOgWoejbr8&feature=youtu.be",code:"",ppt:"https://bitwangyujia.github.io/research/project/Speech_Results/Speech_Results.html",web:"https://bitwangyujia.github.io/research/project/comic2speech.html"}},{title:"A Deep Coarse-to-Fine Network for Head Pose Estimation from Synthetic Data",authors:"Yujia Wang, Wei Liang, Jianbing Shen, Yunde Jia, and Lap-Fai Yu",publisher:"Pattern Recognition 2019",description:"We propose a deep neural network to estimate head poses using the Coarse-to-Fine strategy. To tackle the problem of insufficient annotated data for training, we design a rendering pipeline to synthesize realistic head images and generate an annotated dataset containing 310k head poses.",links:{image:"https://i.loli.net/2020/09/26/Baqm1CbKFkfoT53.gif",paper:"http://liangwei-bit.github.io/web/project/headpose/PR-headpose-2019.pdf",arxiv:"",video:"https://youtu.be/30nTWHXURjQ",code:"",ppt:"http://liangwei-bit.github.io/web/project/headpose/PR-headpose-2019.pdf",web:"http://liangwei-bit.github.io/web/project/headpose"}},{title:"Virtual Agent Positioning Driven by Scene Semantics in Mixed Reality",authors:"Yining Lang, Wei Liang, and Lap-Fai Yu",publisher:"IEEE VR 2019 (CCF A)",description:"We propose a novel approach using scene semantics to guide the positioning of a virtual agent. This approach comprises a visibility term and a spatial term and achieves a higher user evaluation score than the alternative approaches.",links:{image:"https://i.loli.net/2020/09/26/8mMNcpvxXsb9g6j.gif",paper:"http://liangwei-bit.github.io/web/project/agent/AR-v22.pdf",arxiv:"",video:"http://liangwei-bit.github.io/web/project/agent/video.mp4",code:"",ppt:"http://liangwei-bit.github.io/web/project/agent/AR-v22.pdf",web:"http://liangwei-bit.github.io/web/project/agent"}},{title:"Functional Workspace Optimization via Learning Personal Preferences from Virtual Experiences",authors:"Wei Liang, Jingjing Liu, Yining Lang, Bing Ning and Lap-Fai Yu",publisher:"IEEE TVCG 2019 (CCF A)",description:"We propose an approach to learn the personal preferences of using a functional workspace by analyzing a user performing given tasks via VR devices, e.g., making a salad in the kitchen. The learned preferences are applied to optimizing the workspace, which results in an updated layout that fits the user's preferences better.",links:{image:"https://i.loli.net/2020/09/26/nbygOF6vptMuPiY.gif",paper:"http://liangwei-bit.github.io/web/project/kitchen/tvcg19kitchen.pdf",arxiv:"",video:"https://youtu.be/2Eg3_0vHU2U",code:"",ppt:"http://liangwei-bit.github.io/web/project/kitchen/tvcg19kitchen.pdf",web:"http://liangwei-bit.github.io/web/project/kitchen"}},{title:"Deep Single-View 3D Object Reconstruction with Visual Hull Embedding",authors:"Hanqing Wang, Jiaolong Yang, Wei Liang and Xin Tong",publisher:'AAAI 2019 (CCF A), <font style="color:#FF0800">Oral Presentation </font>',description:"We propose a 3D reconstruction approach that preserves more shape details and improves the reconstruction quality. Our key idea is to leverage object mask and pose estimation from CNNs to assist 3D shape learning by constructing a probabilistic single-view visual hull inside the network.",links:{image:"https://i.loli.net/2020/09/26/SC1dtHu6JKarlxR.gif",paper:"",arxiv:"https://arxiv.org/pdf/1809.03451.pdf",video:"https://youtu.be/nBjzM7PasGk",code:"",ppt:"http://liangwei-bit.github.io/web/project/aaai19_3d_recon/presentation.pdf",web:"http://liangwei-bit.github.io/web/project/aaai19_3d_recon"}},{title:"3D Face Synthesis Driven by Personality Impression",authors:"Yining Lang, Wei Liang, Yujia Wang, and Lap-Fai Yu",publisher:"AAAI 2019 (CCF A)",description:"We propose a novel approach to synthesize 3D faces based on personality impressions for creating virtual characters. We demonstrate that our approach can automatically synthesize a variety of 3D faces to give different personality impressions.",links:{image:"https://i.loli.net/2020/09/26/lQUSbprea718hmZ.gif",paper:"http://liangwei-bit.github.io/web/project/face/aaai19-face_v8.pdf",arxiv:"",video:"https://youtu.be/B8b8iQOu45c",code:"",ppt:"http://liangwei-bit.github.io/web/project/face/aaai19-face_v8.pdf",web:"http://liangwei-bit.github.io/web/project/face"}},{title:"Tracking Occluded Objects and Recovering Incomplete Trajectories by Reasoning about Containment Relations and Human Actions",authors:"Wei Liang, Yixin Zhu, and Song-Chun Zhu",publisher:"AAAI 2018 (CCF A)",description:"We propose an algorithm to formulate the tracking problem as a network flow representation encoding containment relations and their changes and demonstrating better performance on tracking occluded objects than baseline methods.",links:{image:"https://i.loli.net/2020/09/26/KqYvHmXf8adlDwV.gif",paper:"http://liangwei-bit.github.io/web/project/aaai_tracking/AAAI_2018_Tracking_by Reasoning.pdf",arxiv:"",video:"https://youtu.be/Uc8pKLo319I",code:"",ppt:"/",web:"http://liangwei-bit.github.io/web/project/aaai_tracking"}},{title:"Synthesizing Personalized Training Programs for Improving Driving Habits via Virtual Reality",authors:"Yining Lang, Wei Liang, Fang Xu, Yibiao Zhao, and Lap-Fai Yu",publisher:"IEEE VR 2018 (CCF A)",description:"We propose an approach first to identify the improper driving habits of a user when he drives in a virtual city. Then, it synthesizes a pertinent training program to help improve the users driving skills based on the discovered improper habits of the user.",links:{image:"https://i.loli.net/2020/09/26/mC1lxOeP4zMDTqk.gif",paper:"http://liangwei-bit.github.io/web/project/Driving/IEEE-VR-DRIVING.pdf",arxiv:"",video:"https://youtu.be/KEIFE8ZfWgo",code:"",ppt:"http://liangwei-bit.github.io/web/project/Driving/IEEE-VR-Driving.pdf",web:"http://liangwei-bit.github.io/web/project/Driving"}},{title:"Spatially Perturbed Collision Sounds Attenuate Perceived Causality in 3D Launching Events",authors:"Duotun Wang, James Kubricht, Yixin Zhu, Wei Liang, Song-Chun Zhu, Chenfanfu Jiang, and Hongjing Lu",publisher:"IEEE VR 2018 (CCF A)",description:"We discover that people can localize sound positions based on the auditory inputs in VR environments, and the spatial discrepancy between the estimated position of the collision sound and the visually observed impact location attenuates perceived causality.",links:{image:"https://i.loli.net/2020/09/26/6WqKGnrNezj7m3P.gif",paper:"http://liangwei-bit.github.io/web/project/Wang/vr18b-sub1099-cam-i6.pdf",arxiv:"",video:"https://youtu.be/s0pPDoyPnCk",code:"",ppt:"http://liangwei-bit.github.io/web/project/Wang/ieeevr2018_oral_final _for pdf.pdf",web:"http://liangwei-bit.github.io/web/project/Wang"}},{title:"Transferring Object: Joint Inference of Container and Human Pose",authors:"Hanqing Wang, Wei Liang, and Lap-Fai Yu",publisher:"ICCV 2017 (CCF A)",description:"We propose an approach to jointly infer container and human pose for transferring objects by minimizing the costs associated with both object and pose candidates.",links:{image:"https://i.loli.net/2020/09/26/hszOl8Aw5u2Z7K6.gif",paper:"http://liangwei-bit.github.io/web/project/container/iccv2017container_files/iccv2017container.pdf",arxiv:"",video:"https://youtu.be/oCNCHbYmGgg",code:"",ppt:"/",web:"http://liangwei-bit.github.io/web/project/container"}},{title:"Earthquake Safety Training through Virtual Drills",authors:"Changyang Li, Wei Liang, Chris Quigley, Yibiao Zhao, and Lap-Fai Yu",publisher:"IEEE TVCG 2017 (CCF A)",description:"We provide an immersive and novel virtual reality training approach designed to teach individuals how to survive earthquakes in indoor environments. Our approach utilizes virtual environments realistically populated with furniture objects for training.",links:{image:"https://i.loli.net/2020/09/26/KcHQkZiz56eMvmp.gif",paper:"http://liangwei-bit.github.io/web/project/earthquake/vr2017earthquake_files/vr2017earthquake.pdf",arxiv:"",video:"https://youtu.be/NdVcUAB0XAE",code:"",ppt:"/",web:"http://liangwei-bit.github.io/web/project/earthquake"}},{title:"What is Where: Inferring Containment Relations from Videos",authors:"Wei Liang, Yibiao Zhao, Yixin Zhu, and Song-chun Zhu",publisher:'IJCAI 2016 (CCF A), <font style="color:#FF0800">Oral Presentation </font>',description:"We present a probabilistic approach to explicitly infer containment relations between objects in 3D scenes. Given an input RGB-D video, our algorithm quantizes the perceptual space of a 3D scene by reasoning about containment relations over time.",links:{image:"https://i.loli.net/2020/09/26/DesYv2wZ9iBqMTA.gif",paper:"http://liangwei-bit.github.io/web/project/ijcai16_container/ijcai16_container.pdf",arxiv:"",video:"https://youtu.be/VQlssiSGQHc",code:"",ppt:"/",web:"http://liangwei-bit.github.io/web/project/ijcai16_container"}},{title:"3D Head Pose Estimation with Convolutional Neural Network Trained on Synthetic Images",authors:"Xiabing Liu, Wei Liang, Yumeng Wang, Shuyang Li, and Mingtao Pei",publisher:"ICIP 2016",description:"We propose a method to estimate head pose using a convolutional neural network trained on synthetic head images. We formulate head pose estimation as a regression problem and evaluate our method on synthetic and real data.",links:{image:"https://i.loli.net/2020/09/26/cZl9IMFHp2iLArs.gif",paper:"http://liangwei-bit.github.io/web/project/icip16_headpose/icip16_headpose.pdf",arxiv:"",video:"/",code:"",ppt:"/",web:"http://liangwei-bit.github.io/web/project/icip16_headpose"}},{title:"Evaluating Human Cognition of Containing Relations with Physical Simulation",authors:"Wei Liang, Yibiao Zhao, Yixin Zhu, and Songchun Zhu",publisher:"CogSci 2015 (CCF B)",description:"We discover that physical simulation is an excellent approximation of the human cognition of containers and the containing relations using physical simulation. We also analyze human judgments concerning physical simulation results under different scenarios.",links:{image:"https://i.loli.net/2020/09/26/843unv9Cjz75m6N.gif",paper:"http://liangwei-bit.github.io/web/project/cogsci15_container/cogsci15_container.pdf",arxiv:"",video:"https://www.youtube.com/watch?v=kmO4MptwhhM",code:"",ppt:"http://liangwei-bit.github.io/web/project/cogsci15_container/ppt.pdf",web:"http://liangwei-bit.github.io/web/project/cogsci15_container"}}],N=["/images/lab/lab-000.jpg","/images/lab/lab-001.jpg","/images/lab/lab-002.jpg","/images/lab/lab-003.jpg","/images/lab/lab-004.jpg","/images/lab/lab-005.jpg","/images/lab/lab-006.jpg","/images/lab/lab-007.jpg","/images/lab/lab-008.jpg","/images/lab/lab-210530-1.jpg","/images/lab/lab-230626-02.jpg","/images/lab/lab-230626-03.jpg","/images/lab/lab-230626-04.jpg","/images/lab/lab-230626-07.jpg","/images/lab/lab-230626-08.jpg","/images/lab/lab-230626-09.jpg","/images/lab/lab-230626-10.jpg","/images/lab/lab-230626-11.jpg","/images/lab/lab-230626-12.jpg","/images/lab/lab-241012-02.jpg","/images/lab/lab-250822-01.jpg","/images/lab/lab-250822-02.jpg","/images/lab/lab-250822-03.jpg","/images/lab/lab-250822-05.jpg","/images/lab/lab-250822-06.jpg"],ge={id:"home"},he={class:"max-w-7xl px-4 sm:px-6 lg:px-8 mx-auto"},me={id:"news"},be={class:"max-w-7xl px-4 lg:px-8 py-12 lg:py-16 mx-auto"},ve={class:"px-6 md:px-12"},fe={class:"list-disc list-inside"},we={class:"text-lg font-semibold"},xe=["innerHTML"],ye={id:"publications"},ke={class:"max-w-7xl px-4 sm:px-6 lg:px-8 py-12 lg:py-16 mx-auto"},Ce={class:"flex flex-col justify-center items-center"},je={id:"team"},Ne={class:"max-w-7xl px-4 sm:px-6 lg:px-8 py-12 lg:py-16 mx-auto"},Se={class:"flex flex-wrap justify-center gap-6"},Le={key:0,class:"text-center text-2xl font-semibold pt-12 pb-6"},Ae={key:1,class:"flex flex-wrap justify-center gap-6"},We={key:2,class:"text-center text-2xl font-semibold pt-12 pb-6"},Me={key:3,class:"flex flex-wrap justify-center gap-6"},_e={key:4,class:"text-center text-2xl font-semibold pt-12 pb-6"},Ee={key:5,class:"flex flex-wrap justify-center gap-6"},Ie={key:6,class:"text-center text-2xl font-semibold pt-12 pb-3"},Ve={key:7,class:"flex flex-wrap justify-center gap-6 pb-6"},Re={key:8,id:"hs-show-hide-collapse-heading",class:"hs-collapse hidden w-full overflow-hidden transition-[height] duration-300 flex flex-wrap justify-center gap-6"},Pe={key:9,class:"mt-6 text-center"},Fe={id:"photos"},He={class:"w-[100vdw] py-12 lg:py-16 mx-auto"},Te={class:"overflow-hidden w-full py-5"},Ye={class:"flex gap-6 animate-slide"},ze=["src"],De=["src"],qe={class:"overflow-hidden w-full py-5"},Oe={class:"flex gap-6 animate-slide"},Ze=["src"],Ge=["src"],Je={class:"w-full max-w-7xl py-6 md:pt-0 px-4 sm:px-6 lg:px-8 mx-auto"},Be={class:"grid grid-cols-1 items-center gap-5 py-6"},Xe={class:"text-center text-neutral-700 dark:text-neutral-300"},$e={class:"pb-3"},Qe="/images/teaser-1.comp.jpg",Ue={__name:"Main",setup(r){const l=I(!1),s=()=>{l.value=window.scrollY>12};V(()=>{window.addEventListener("scroll",s),pe("#hs-header-links",{sectionSelector:"section",targetSelector:"a",offset:64,activeClass:"active-nav-link"})}),R(()=>{window.removeEventListener("scroll",s)});const m=b(()=>ue.slice(0,10).sort((p,o)=>new Date(o.date)-new Date(p.date))),u=b(()=>k.filter(p=>p.role===e.faculty).sort((p,o)=>p.enName.split(" ").reverse().join(" ").localeCompare(o.enName.split(" ").reverse().join(" ")))),v=b(()=>k.filter(p=>p.role===e.student&&p.description.toLowerCase().includes("phd")).sort((p,o)=>p.enName.split(" ").reverse().join(" ").localeCompare(o.enName.split(" ").reverse().join(" ")))),C=b(()=>k.filter(p=>p.role===e.student&&p.description.toLowerCase().includes("master")).sort((p,o)=>p.enName.split(" ").reverse().join(" ").localeCompare(o.enName.split(" ").reverse().join(" ")))),S=b(()=>k.filter(p=>p.role===e.undergraduate).sort((p,o)=>p.enName.split(" ").reverse().join(" ").localeCompare(o.enName.split(" ").reverse().join(" ")))),x=b(()=>{let p=k.filter(i=>i.role===e.alumni&&i.description.toLowerCase().includes("phd")).sort((i,c)=>i.enName.split(" ").reverse().join(" ").localeCompare(c.enName.split(" ").reverse().join(" "))),o=k.filter(i=>i.role===e.alumni&&i.description.toLowerCase().includes("master")).sort((i,c)=>i.enName.split(" ").reverse().join(" ").localeCompare(c.enName.split(" ").reverse().join(" ")));return[...p,...o]}),_=b(()=>de.slice(0,5)),L=b(()=>N.slice(0,Math.ceil(N.length/2))),A=b(()=>N.slice(Math.ceil(N.length/2))),E=new Date().getFullYear();return(p,o)=>(a(),n("div",{class:M([{scrolled:l.value},"bg-white dark:bg-neutral-900 text-black dark:text-white"])},[o[12]||(o[12]=W('<header class="fixed top-0 left-0 flex flex-wrap z-50 w-full py-6 lg:justify-start lg:flex-nowrap translate-y-0 transition-all duration-300"><nav class="relative max-w-7xl w-full flex flex-wrap lg:grid lg:grid-cols-12 basis-full items-center px-4 md:px-6 lg:px-8 mx-auto"><div class="lg:col-span-3 flex items-center header-left transition-all duration-100"><a class="flex-none rounded-xl text-2xl inline-block font-semibold" href="/" aria-label="Preline"><img class="inline-block size-12.5 rounded-lg" src="'+T+'" alt="Logo"><span class="inline-block ms-3 mt-3">PIE Lab</span></a></div><div class="flex items-center gap-x-1 lg:gap-x-2 ms-auto py-1 lg:ps-6 lg:order-3 lg:col-span-3 header-right transition-all duration-100"><button type="button" class="hs-dark-mode hs-dark-mode-active:hidden flex items-center gap-x-2 py-2 px-3 bg-white border border-gray-200 rounded-full text-sm text-black hover:bg-neutral-100" data-hs-theme-click-value="dark"><svg class="shrink-0 size-4" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 3a6 6 0 0 0 9 9 9 9 0 1 1-9-9Z"></path></svg> Dark </button><button type="button" class="hs-dark-mode hs-dark-mode-active:inline-flex hidden items-center gap-x-2 py-2 px-3 bg-white/10 rounded-full text-sm text-white hover:bg-white/20" data-hs-theme-click-value="light"><svg class="shrink-0 size-4" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="4"></circle><path d="M12 2v2"></path><path d="M12 20v2"></path><path d="m4.93 4.93 1.41 1.41"></path><path d="m17.66 17.66 1.41 1.41"></path><path d="M2 12h2"></path><path d="M20 12h2"></path><path d="m6.34 17.66-1.41 1.41"></path><path d="m19.07 4.93-1.41 1.41"></path></svg> Light </button><div class="lg:hidden"><button type="button" id="hs-header-collapse-button" data-hs-collapse="#hs-header-collapse" class="hs-collapse-toggle size-9.5 flex justify-center items-center text-sm font-semibold rounded-xl border border-gray-200 text-black hover:bg-neutral-100 dark:text-white dark:border-neutral-700 dark:hover:bg-neutral-700"><svg class="hs-collapse-open:hidden shrink-0 size-4" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="3" x2="21" y1="6" y2="6"></line><line x1="3" x2="21" y1="12" y2="12"></line><line x1="3" x2="21" y1="18" y2="18"></line></svg><svg class="hs-collapse-open:block hidden shrink-0 size-4" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M18 6 6 18"></path><path d="m6 6 12 12"></path></svg></button></div></div><div id="hs-header-collapse" class="hs-collapse hidden overflow-hidden basis-full py-3 grow lg:block lg:w-auto lg:basis-auto lg:order-2 lg:col-span-6 header-center transition-all duration-300"><div id="hs-header-links" class="flex flex-col gap-y-4 gap-x-0 mt-5 lg:flex-row lg:justify-center lg:items-center lg:gap-y-0 lg:gap-x-7 lg:mt-0"><a class="inline-block text-black hover:text-gray-500 dark:hover:text-neutral-300; dark:text-white active-nav-link" href="#home">Home</a><a class="inline-block text-black hover:text-gray-500 dark:hover:text-neutral-300; dark:text-white" href="#news">News</a><a class="inline-block text-black hover:text-gray-500 dark:hover:text-neutral-300; dark:text-white" href="#publications">Publications</a><a class="inline-block text-black hover:text-gray-500 dark:hover:text-neutral-300; dark:text-white" href="#team">Team</a><a class="inline-block text-black hover:text-gray-500 dark:hover:text-neutral-300; dark:text-white" href="#photos">Photos</a></div></div></nav></header>',1)),t("section",ge,[o[1]||(o[1]=t("div",{class:"h-32"},null,-1)),t("div",he,[t("div",{style:P({backgroundImage:`url(${Qe})`}),class:"h-120 md:h-[80dvh] 2xl:h-auto 2xl:aspect-[2/1] w-full flex flex-col bg-cover bg-center bg-no-repeat rounded-2xl"},o[0]||(o[0]=[t("div",{class:"mt-auto w-2/3 md:max-w-lg ps-5 pb-5 md:ps-10 md:pb-10"},[t("h1",{class:"text-xl md:text-3xl lg:text-5xl text-white text-yellow-400"}," PIE Lab @ BIT ")],-1)]),4)]),o[2]||(o[2]=W('<div class="max-w-7xl px-6 lg:px-12 py-3 lg:py-6 mx-auto"><blockquote class="relative ms-6 p-6 mt-12 mb-6 rounded-xl"><svg class="absolute -top-6 -start-8 size-16 text-gray-100 dark:text-neutral-700" width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg" aria-hidden="true"><path d="M7.39762 10.3C7.39762 11.0733 7.14888 11.7 6.6514 12.18C6.15392 12.6333 5.52552 12.86 4.76621 12.86C3.84979 12.86 3.09047 12.5533 2.48825 11.94C1.91222 11.3266 1.62421 10.4467 1.62421 9.29999C1.62421 8.07332 1.96459 6.87332 2.64535 5.69999C3.35231 4.49999 4.33418 3.55332 5.59098 2.85999L6.4943 4.25999C5.81354 4.73999 5.26369 5.27332 4.84476 5.85999C4.45201 6.44666 4.19017 7.12666 4.05926 7.89999C4.29491 7.79332 4.56983 7.73999 4.88403 7.73999C5.61716 7.73999 6.21938 7.97999 6.69067 8.45999C7.16197 8.93999 7.39762 9.55333 7.39762 10.3ZM14.6242 10.3C14.6242 11.0733 14.3755 11.7 13.878 12.18C13.3805 12.6333 12.7521 12.86 11.9928 12.86C11.0764 12.86 10.3171 12.5533 9.71484 11.94C9.13881 11.3266 8.85079 10.4467 8.85079 9.29999C8.85079 8.07332 9.19117 6.87332 9.87194 5.69999C10.5789 4.49999 11.5608 3.55332 12.8176 2.85999L13.7209 4.25999C13.0401 4.73999 12.4903 5.27332 12.0713 5.85999C11.6786 6.44666 11.4168 7.12666 11.2858 7.89999C11.5215 7.79332 11.7964 7.73999 12.1106 7.73999C12.8437 7.73999 13.446 7.97999 13.9173 8.45999C14.3886 8.93999 14.6242 9.55333 14.6242 10.3Z" fill="currentColor"></path></svg><div class="relative z-10"><p class="text-lg text-justify"><em> The <span class="font-semibold">Perception, Interaction, and Embodiment Lab (PIE Lab)</span>, affiliated with the <a class="simple-link" href="https://cs.bit.edu.cn/">School of Computer Science &amp; Technology</a> at <a class="simple-link" href="https://bit.edu.cn/">Beijing Institute of Technology (BIT)</a>, aiming to build intelligent systems that see, interact, and act in the world. </em></p></div></blockquote><div class="grid grid-cols-1 lg:grid-cols-3 gap-8"><div class="flex flex-col bg-white border border-gray-200 shadow-2xs rounded-xl p-4 md:p-5 dark:bg-neutral-900 dark:border-neutral-700 dark:shadow-neutral-700/70"><h3 class="text-lg font-bold text-gray-800 dark:text-white"> Perception </h3><p class="mt-2 text-gray-500 dark:text-neutral-400 text-justify"> We focus on multi-sensor perception and fusion, integrating vision, LiDAR, IMU, and other modalities to support applications in unmanned platforms. </p></div><div class="flex flex-col bg-white border border-gray-200 shadow-2xs rounded-xl p-4 md:p-5 dark:bg-neutral-900 dark:border-neutral-700 dark:shadow-neutral-700/70"><h3 class="text-lg font-bold text-gray-800 dark:text-white"> Interaction </h3><p class="mt-2 text-gray-500 dark:text-neutral-400 text-justify"> We explore human-computer interaction, with an emphasis on intelligent content generation for VR/AR applications to enhance immersive experiences. </p></div><div class="flex flex-col bg-white border border-gray-200 shadow-2xs rounded-xl p-4 md:p-5 dark:bg-neutral-900 dark:border-neutral-700 dark:shadow-neutral-700/70"><h3 class="text-lg font-bold text-gray-800 dark:text-white"> Embodiment </h3><p class="mt-2 text-gray-500 dark:text-neutral-400 text-justify"> We aim to endow embodied AI agents with fundamental motor skills, empowering navigation and locomotion for both wheeled and legged robots. </p></div></div></div>',1))]),t("section",me,[t("div",be,[o[3]||(o[3]=t("div",{class:"mb-6 sm:mb-10 max-w-2xl text-center mx-auto"},[t("h1",{class:"font-medium text-black text-3xl sm:text-4xl dark:text-white"}," Recent News ")],-1)),t("div",ve,[t("ul",fe,[(a(!0),n(g,null,h(m.value,(i,c)=>(a(),n("li",{key:c,class:"mb-2"},[t("span",we,"["+f(i.date)+"]",1),t("span",{class:"text-lg ms-2",innerHTML:i.content},null,8,xe)]))),128))])])])]),t("section",ye,[t("div",ke,[o[4]||(o[4]=t("div",{class:"mb-6 sm:mb-10 max-w-2xl text-center mx-auto"},[t("h1",{class:"font-medium text-black text-3xl sm:text-4xl dark:text-white"}," Selected Publications ")],-1)),t("div",Ce,[(a(!0),n(g,null,h(_.value,(i,c)=>(a(),w(le,{key:c,title:i.title,authors:i.authors,publisher:i.publisher,links:i.links},null,8,["title","authors","publisher","links"]))),128))]),o[5]||(o[5]=t("div",{class:"mt-6 text-center"},[t("a",{class:"inline-flex items-center gap-x-1 text-sm decoration-2 font-semibold simple-link",href:"/publication"},[j(" Full publication list "),t("svg",{class:"shrink-0 size-4",xmlns:"http://www.w3.org/2000/svg",width:"24",height:"24",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor","stroke-width":"2","stroke-linecap":"round","stroke-linejoin":"round"},[t("path",{d:"m9 18 6-6-6-6"})])])],-1))])]),t("section",je,[t("div",Ne,[o[7]||(o[7]=t("div",{class:"mb-6 sm:mb-10 max-w-2xl text-center mx-auto"},[t("h1",{class:"font-medium text-black text-3xl sm:text-4xl dark:text-white"}," Team Members ")],-1)),o[8]||(o[8]=t("div",{class:"text-center text-2xl font-semibold pt-3 pb-6"}," Faculty ",-1)),t("div",Se,[(a(!0),n(g,null,h(u.value,(i,c)=>(a(),w(y,{key:c,enName:i.enName,chName:i.chName,description:i.description,links:i.links},null,8,["enName","chName","description","links"]))),128))]),v.value.length>0?(a(),n("div",Le," PhD Student ")):d("",!0),v.value.length>0?(a(),n("div",Ae,[(a(!0),n(g,null,h(v.value,(i,c)=>(a(),w(y,{key:c,enName:i.enName,chName:i.chName,description:i.description,links:i.links},null,8,["enName","chName","description","links"]))),128))])):d("",!0),C.value.length>0?(a(),n("div",We," Master Student ")):d("",!0),C.value.length>0?(a(),n("div",Me,[(a(!0),n(g,null,h(C.value,(i,c)=>(a(),w(y,{key:c,enName:i.enName,chName:i.chName,description:i.description,links:i.links},null,8,["enName","chName","description","links"]))),128))])):d("",!0),S.value.length>0?(a(),n("div",_e," Undergrad Student ")):d("",!0),S.value.length>0?(a(),n("div",Ee,[(a(!0),n(g,null,h(S.value,(i,c)=>(a(),w(y,{key:c,enName:i.enName,chName:i.chName,description:i.description,links:i.links},null,8,["enName","chName","description","links"]))),128))])):d("",!0),x.value.length>0?(a(),n("div",Ie," Alumni ")):d("",!0),x.value.length>0?(a(),n("div",Ve,[(a(!0),n(g,null,h(x.value.slice(0,5),(i,c)=>(a(),w(y,{key:c,isAlumni:!0,enName:i.enName,chName:i.chName,description:i.description,links:i.links},null,8,["enName","chName","description","links"]))),128))])):d("",!0),x.value.length>5?(a(),n("div",Re,[(a(!0),n(g,null,h(x.value.slice(5),(i,c)=>(a(),w(y,{key:c,isAlumni:!0,enName:i.enName,chName:i.chName,description:i.description,links:i.links},null,8,["enName","chName","description","links"]))),128))])):d("",!0),x.value.length>5?(a(),n("p",Pe,o[6]||(o[6]=[W('<button type="button" class="hs-collapse-toggle inline-flex items-center gap-x-1 text-sm decoration-2 font-semibold simple-link" id="hs-show-hide-collapse" data-hs-collapse="#hs-show-hide-collapse-heading"><span class="hs-collapse-open:hidden">Show more</span><span class="hs-collapse-open:block hidden">Show less</span><svg class="hs-collapse-open:rotate-180 shrink-0 size-4" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="m6 9 6 6 6-6"></path></svg></button>',1)]))):d("",!0)])]),t("section",Fe,[t("div",He,[o[9]||(o[9]=t("div",{class:"mb-6 sm:mb-10 max-w-2xl text-center mx-auto"},[t("h1",{class:"font-medium text-black text-3xl sm:text-4xl dark:text-white"}," Lab Photos ")],-1)),t("div",Te,[t("div",Ye,[(a(!0),n(g,null,h(L.value,(i,c)=>(a(),n("img",{key:c,src:i,class:"rounded-xl h-48 xl:h-64 3xl:h-80 w-auto"},null,8,ze))),128)),(a(!0),n(g,null,h(L.value,(i,c)=>(a(),n("img",{key:c+2*L.value.length,src:i,class:"rounded-xl h-48 xl:h-64 3xl:h-80 w-auto"},null,8,De))),128))])]),t("div",qe,[t("div",Oe,[(a(!0),n(g,null,h(A.value,(i,c)=>(a(),n("img",{key:c,src:i,class:"rounded-xl h-48 xl:h-64 3xl:h-80 w-auto"},null,8,Ze))),128)),(a(!0),n(g,null,h(A.value,(i,c)=>(a(),n("img",{key:c+2*A.value.length,src:i,class:"rounded-xl h-48 xl:h-64 3xl:h-80 w-auto"},null,8,Ge))),128))])])])]),t("footer",Je,[o[11]||(o[11]=t("hr",{class:"border-0.5 border-neutral-200 dark:border-neutral-800"},null,-1)),t("div",Be,[t("div",Xe,[t("div",$e,"© "+f(F(E))+" PIE Lab @ BIT.",1),o[10]||(o[10]=t("div",{class:"pb-3 text-sm"},[j("The site is deployed on "),t("a",{class:"simple-link",href:"https://pages.github.com/"},"Github Pages"),j(". The source is licensed under "),t("a",{class:"simple-link",href:"https://creativecommons.org/licenses/by-nc-nd/4.0/"}," CC-BY-NC_ND"),j(".")],-1))])])])],2))}};H(Ue).mount("#app");
